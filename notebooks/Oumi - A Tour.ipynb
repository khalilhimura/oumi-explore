{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khalilhimura/oumi-explore/blob/main/notebooks/Oumi%20-%20A%20Tour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm2uIsXvh2U9"
      },
      "source": [
        "<div class=\"align-center\">\n",
        "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
        "\n",
        "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
        "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
        "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
        "</div>\n",
        "\n",
        "üëã Welcome to Open Universal Machine Intelligence (Oumi)!\n",
        "\n",
        "üöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](hhttps://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
        "\n",
        "ü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
        "\n",
        "‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7bYaH10SgtN"
      },
      "source": [
        "# A Tour of Oumi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkhNGqE1SgtP"
      },
      "source": [
        "This tutorial will give you a brief overview of Oumi's core functionality. We'll cover:\n",
        "\n",
        "1. Training a model\n",
        "1. Performing model inference\n",
        "1. Evaluating a model against common benchmarks\n",
        "1. Launching jobs\n",
        "1. Customizing datasets and clouds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHDr11SqSgtP"
      },
      "source": [
        "# üìã Prerequisites\n",
        "## Oumi Installation\n",
        "\n",
        "First, let's install Oumi. You can find more detailed instructions [here](https://oumi.ai/docs/en/latest/get_started/installation.html).\n",
        "\n",
        "If you have a GPU, you can run the following commands to install Oumi:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ru9J1Z2uh2VB",
        "outputId": "74daad04-1948-4915-b490-354fa67ec4ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mResolved \u001b[1m144 packages\u001b[0m \u001b[2min 550ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m14 packages\u001b[0m \u001b[2min 679ms\u001b[0m\u001b[0m\n",
            "\u001b[2mInstalled \u001b[1m14 packages\u001b[0m \u001b[2min 352ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.1.3.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.0.2.54\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.2.106\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.4.5.107\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.1.0.106\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.20.5\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==10.3.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.4.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.19.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.0.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%pip install uv -q\n",
        "!uv pip install oumi --no-progress --system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JPmWKRVCSgtP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "tutorial_dir = \"tour_tutorial\"\n",
        "\n",
        "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable warnings from HF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8PTJuc4SgtQ"
      },
      "source": [
        "# ‚öíÔ∏è Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2_HamuySgtQ"
      },
      "source": [
        "Oumi supports training both custom and out-of-the-box models. Want to try out a model on HuggingFace? You can do that. Want to train your own custom Pytorch model? No problem.\n",
        "\n",
        "## A Quick Demo\n",
        "\n",
        "Let's try training a pre-existing model on HuggingFace. We'll use SmolLM2 135M as it's small and trains quickly.\n",
        "\n",
        "Oumi uses [training configuration files](https://oumi.ai/docs/en/latest/api/oumi.core.configs.html#oumi.core.configs.TrainingConfig) to specify training parameters. We've already created a training config for SmolLM2 ‚Äî let's give it a try!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l2SQ9fZiSgtQ"
      },
      "outputs": [],
      "source": [
        "yaml_content = f\"\"\"\n",
        "model:\n",
        "  model_name: \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  trust_remote_code: True\n",
        "\n",
        "data:\n",
        "  train:\n",
        "    datasets:\n",
        "      - dataset_name: \"yahma/alpaca-cleaned\"\n",
        "    target_col: \"prompt\"\n",
        "\n",
        "training:\n",
        "  trainer_type: \"TRL_SFT\"\n",
        "  per_device_train_batch_size: 2\n",
        "  max_steps: 10 # Quick \"mini\" training, for demo purposes only.\n",
        "  run_name: \"smollm2_135m_sft\"\n",
        "  output_dir: \"{tutorial_dir}/output\"\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{tutorial_dir}/train.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2GpQDGG5SgtQ",
        "outputId": "6f820933-e7fe-4b09-a643-4e6d4e68c229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-01-30 09:44:55,279][oumi][rank0][pid:4366][MainThread][INFO]][torch_utils.py:66] Torch version: 2.6.0+cu124. NumPy version: 1.26.4\n",
            "[2025-01-30 09:44:55,313][oumi][rank0][pid:4366][MainThread][INFO]][torch_utils.py:72] CUDA version: 12.4 CuDNN version: 90.1.0\n",
            "[2025-01-30 09:44:55,469][oumi][rank0][pid:4366][MainThread][INFO]][torch_utils.py:106] CPU cores: 2 CUDA devices: 1\n",
            "device(0)='Tesla T4' Capability: (7, 5) Memory: [Total: 14.74GiB Free: 14.64GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
            "[2025-01-30 09:44:55,482][oumi][rank0][pid:4366][MainThread][INFO]][train.py:133] Oumi version: 0.1.3\n",
            "[2025-01-30 09:44:55,490][oumi][rank0][pid:4366][MainThread][INFO]][train.py:174] TrainingConfig:\n",
            "TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='yahma/alpaca-cleaned',\n",
            "                                                                                dataset_path=None,\n",
            "                                                                                subset=None,\n",
            "                                                                                split='train',\n",
            "                                                                                dataset_kwargs={},\n",
            "                                                                                sample_count=None,\n",
            "                                                                                mixture_proportion=None,\n",
            "                                                                                shuffle=False,\n",
            "                                                                                seed=None,\n",
            "                                                                                shuffle_buffer_size=1000,\n",
            "                                                                                trust_remote_code=False,\n",
            "                                                                                transform_num_workers=None)],\n",
            "                                                        collator_name=None,\n",
            "                                                        pack=False,\n",
            "                                                        stream=False,\n",
            "                                                        target_col='prompt',\n",
            "                                                        mixture_strategy='first_exhausted',\n",
            "                                                        seed=None,\n",
            "                                                        use_async_dataset=False,\n",
            "                                                        use_torchdata=None),\n",
            "                               test=DatasetSplitParams(datasets=[],\n",
            "                                                       collator_name=None,\n",
            "                                                       pack=False,\n",
            "                                                       stream=False,\n",
            "                                                       target_col=None,\n",
            "                                                       mixture_strategy='first_exhausted',\n",
            "                                                       seed=None,\n",
            "                                                       use_async_dataset=False,\n",
            "                                                       use_torchdata=None),\n",
            "                               validation=DatasetSplitParams(datasets=[],\n",
            "                                                             collator_name=None,\n",
            "                                                             pack=False,\n",
            "                                                             stream=False,\n",
            "                                                             target_col=None,\n",
            "                                                             mixture_strategy='first_exhausted',\n",
            "                                                             seed=None,\n",
            "                                                             use_async_dataset=False,\n",
            "                                                             use_torchdata=None)),\n",
            "               model=ModelParams(model_name='HuggingFaceTB/SmolLM2-135M-Instruct',\n",
            "                                 adapter_model=None,\n",
            "                                 tokenizer_name=None,\n",
            "                                 tokenizer_pad_token=None,\n",
            "                                 tokenizer_kwargs={},\n",
            "                                 model_max_length=None,\n",
            "                                 load_pretrained_weights=True,\n",
            "                                 trust_remote_code=True,\n",
            "                                 torch_dtype_str='bfloat16',\n",
            "                                 compile=False,\n",
            "                                 chat_template=None,\n",
            "                                 attn_implementation=None,\n",
            "                                 device_map='auto',\n",
            "                                 model_kwargs={},\n",
            "                                 enable_liger_kernel=False,\n",
            "                                 shard_for_eval=False,\n",
            "                                 freeze_layers=[]),\n",
            "               training=TrainingParams(use_peft=False,\n",
            "                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,\n",
            "                                       enable_gradient_checkpointing=False,\n",
            "                                       gradient_checkpointing_kwargs={},\n",
            "                                       output_dir='tour_tutorial/output',\n",
            "                                       per_device_train_batch_size=2,\n",
            "                                       per_device_eval_batch_size=8,\n",
            "                                       gradient_accumulation_steps=1,\n",
            "                                       max_steps=10,\n",
            "                                       num_train_epochs=3,\n",
            "                                       save_epoch=False,\n",
            "                                       save_steps=500,\n",
            "                                       save_final_model=True,\n",
            "                                       seed=42,\n",
            "                                       run_name='smollm2_135m_sft',\n",
            "                                       metrics_function=None,\n",
            "                                       log_level='info',\n",
            "                                       dep_log_level='warning',\n",
            "                                       enable_wandb=False,\n",
            "                                       enable_tensorboard=True,\n",
            "                                       logging_strategy='steps',\n",
            "                                       logging_dir=None,\n",
            "                                       logging_steps=50,\n",
            "                                       logging_first_step=False,\n",
            "                                       eval_strategy='no',\n",
            "                                       eval_steps=500,\n",
            "                                       learning_rate=5e-05,\n",
            "                                       lr_scheduler_type='linear',\n",
            "                                       lr_scheduler_kwargs={},\n",
            "                                       warmup_ratio=None,\n",
            "                                       warmup_steps=None,\n",
            "                                       optimizer='adamw_torch',\n",
            "                                       weight_decay=0.0,\n",
            "                                       adam_beta1=0.9,\n",
            "                                       adam_beta2=0.999,\n",
            "                                       adam_epsilon=1e-08,\n",
            "                                       sgd_momentum=0.0,\n",
            "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
            "                                       compile=False,\n",
            "                                       include_performance_metrics=False,\n",
            "                                       include_alternative_mfu_metrics=False,\n",
            "                                       log_model_summary=False,\n",
            "                                       resume_from_checkpoint=None,\n",
            "                                       try_resume_from_last_checkpoint=False,\n",
            "                                       dataloader_num_workers=0,\n",
            "                                       dataloader_prefetch_factor=None,\n",
            "                                       dataloader_main_process_only=None,\n",
            "                                       ddp_find_unused_parameters=None,\n",
            "                                       max_grad_norm=1.0,\n",
            "                                       trainer_kwargs={},\n",
            "                                       profiler=ProfilerParams(save_dir=None,\n",
            "                                                               enable_cpu_profiling=False,\n",
            "                                                               enable_cuda_profiling=False,\n",
            "                                                               record_shapes=False,\n",
            "                                                               profile_memory=False,\n",
            "                                                               with_stack=False,\n",
            "                                                               with_flops=False,\n",
            "                                                               with_modules=False,\n",
            "                                                               row_limit=50,\n",
            "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
            "                                                                                               wait=0,\n",
            "                                                                                               warmup=1,\n",
            "                                                                                               active=3,\n",
            "                                                                                               repeat=1,\n",
            "                                                                                               skip_first=1)),\n",
            "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
            "                                                                 collect_telemetry_for_all_ranks=False,\n",
            "                                                                 track_gpu_temperature=False),\n",
            "                                       empty_device_cache_steps=None,\n",
            "                                       nccl_default_timeout_minutes=None),\n",
            "               peft=PeftParams(lora_r=8,\n",
            "                               lora_alpha=8,\n",
            "                               lora_dropout=0.0,\n",
            "                               lora_target_modules=None,\n",
            "                               lora_modules_to_save=None,\n",
            "                               lora_bias='none',\n",
            "                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,\n",
            "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
            "                               q_lora=False,\n",
            "                               q_lora_bits=4,\n",
            "                               bnb_4bit_quant_type='fp4',\n",
            "                               use_bnb_nested_quant=False,\n",
            "                               bnb_4bit_quant_storage='uint8',\n",
            "                               bnb_4bit_compute_dtype='float32',\n",
            "                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),\n",
            "               fsdp=FSDPParams(enable_fsdp=False,\n",
            "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
            "                               cpu_offload=False,\n",
            "                               mixed_precision=None,\n",
            "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
            "                               forward_prefetch=False,\n",
            "                               use_orig_params=None,\n",
            "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
            "                               auto_wrap_policy=<AutoWrapPolicy.NO_WRAP: 'NO_WRAP'>,\n",
            "                               min_num_params=100000,\n",
            "                               transformer_layer_cls=None,\n",
            "                               sync_module_states=True))\n",
            "[2025-01-30 09:44:57,324][oumi][rank0][pid:4366][MainThread][INFO]][models.py:185] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-01-30 09:44:57,326][oumi][rank0][pid:4366][MainThread][INFO]][models.py:255] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
            "[2025-01-30 09:44:58,155][oumi][rank0][pid:4366][MainThread][INFO]][base_map_dataset.py:68] Creating map dataset (type: AlpacaDataset) dataset_name: 'yahma/alpaca-cleaned', dataset_path: 'None'...\n",
            "[2025-01-30 09:44:59,819][oumi][rank0][pid:4366][MainThread][INFO]][base_map_dataset.py:472] Dataset Info:\n",
            "\tSplit: train\n",
            "\tVersion: 0.0.0\n",
            "\tDataset size: 40283906\n",
            "\tDownload size: 44307561\n",
            "\tSize: 84591467 bytes\n",
            "\tRows: 51760\n",
            "\tColumns: ['output', 'input', 'instruction']\n",
            "[2025-01-30 09:45:00,284][oumi][rank0][pid:4366][MainThread][INFO]][base_map_dataset.py:411] Loaded DataFrame with shape: (51760, 3). Columns:\n",
            "output         object\n",
            "input          object\n",
            "instruction    object\n",
            "dtype: object\n",
            "[2025-01-30 09:45:00,328][oumi][rank0][pid:4366][MainThread][INFO]][base_map_dataset.py:297] AlpacaDataset: features=dict_keys(['input_ids', 'attention_mask'])\n",
            "[2025-01-30 09:45:00,830][oumi][rank0][pid:4366][MainThread][INFO]][base_map_dataset.py:361] Finished transforming dataset (AlpacaDataset)! Speed: 103234.84 examples/sec. Examples: 51760. Duration: 0.5 sec. Transform workers: 1.\n",
            "[2025-01-30 09:45:01,209][oumi][rank0][pid:4366][MainThread][INFO]][torch_profiler_utils.py:150] PROF: Torch Profiler disabled!\n",
            "[2025-01-30 09:45:01,233][oumi][rank0][pid:4366][MainThread][INFO]][training.py:49] SFTConfig(output_dir='tour_tutorial/output',\n",
            "          overwrite_output_dir=False,\n",
            "          do_train=False,\n",
            "          do_eval=False,\n",
            "          do_predict=False,\n",
            "          eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
            "          prediction_loss_only=False,\n",
            "          per_device_train_batch_size=2,\n",
            "          per_device_eval_batch_size=8,\n",
            "          per_gpu_train_batch_size=None,\n",
            "          per_gpu_eval_batch_size=None,\n",
            "          gradient_accumulation_steps=1,\n",
            "          eval_accumulation_steps=None,\n",
            "          eval_delay=0,\n",
            "          torch_empty_cache_steps=None,\n",
            "          learning_rate=5e-05,\n",
            "          weight_decay=0.0,\n",
            "          adam_beta1=0.9,\n",
            "          adam_beta2=0.999,\n",
            "          adam_epsilon=1e-08,\n",
            "          max_grad_norm=1.0,\n",
            "          num_train_epochs=3,\n",
            "          max_steps=10,\n",
            "          lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>,\n",
            "          lr_scheduler_kwargs={},\n",
            "          warmup_ratio=0.0,\n",
            "          warmup_steps=0,\n",
            "          log_level='warning',\n",
            "          log_level_replica='warning',\n",
            "          log_on_each_node=True,\n",
            "          logging_dir='tour_tutorial/output/runs/Jan30_09-45-01_fc2e07b569e0',\n",
            "          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
            "          logging_first_step=False,\n",
            "          logging_steps=50,\n",
            "          logging_nan_inf_filter=True,\n",
            "          save_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
            "          save_steps=500,\n",
            "          save_total_limit=None,\n",
            "          save_safetensors=True,\n",
            "          save_on_each_node=False,\n",
            "          save_only_model=False,\n",
            "          restore_callback_states_from_checkpoint=False,\n",
            "          no_cuda=False,\n",
            "          use_cpu=False,\n",
            "          use_mps_device=False,\n",
            "          seed=42,\n",
            "          data_seed=None,\n",
            "          jit_mode_eval=False,\n",
            "          use_ipex=False,\n",
            "          bf16=False,\n",
            "          fp16=False,\n",
            "          fp16_opt_level='O1',\n",
            "          half_precision_backend='auto',\n",
            "          bf16_full_eval=False,\n",
            "          fp16_full_eval=False,\n",
            "          tf32=None,\n",
            "          local_rank=0,\n",
            "          ddp_backend=None,\n",
            "          tpu_num_cores=None,\n",
            "          tpu_metrics_debug=False,\n",
            "          debug=[],\n",
            "          dataloader_drop_last=False,\n",
            "          eval_steps=500,\n",
            "          dataloader_num_workers=0,\n",
            "          dataloader_prefetch_factor=None,\n",
            "          past_index=-1,\n",
            "          run_name='smollm2_135m_sft',\n",
            "          disable_tqdm=False,\n",
            "          remove_unused_columns=True,\n",
            "          label_names=None,\n",
            "          load_best_model_at_end=False,\n",
            "          metric_for_best_model=None,\n",
            "          greater_is_better=None,\n",
            "          ignore_data_skip=False,\n",
            "          fsdp=[],\n",
            "          fsdp_min_num_params=0,\n",
            "          fsdp_config={'min_num_params': 0,\n",
            "                       'xla': False,\n",
            "                       'xla_fsdp_grad_ckpt': False,\n",
            "                       'xla_fsdp_v2': False},\n",
            "          fsdp_transformer_layer_cls_to_wrap=None,\n",
            "          accelerator_config=AcceleratorConfig(split_batches=False,\n",
            "                                               dispatch_batches=None,\n",
            "                                               even_batches=True,\n",
            "                                               use_seedable_sampler=True,\n",
            "                                               non_blocking=False,\n",
            "                                               gradient_accumulation_kwargs=None,\n",
            "                                               use_configured_state=False),\n",
            "          deepspeed=None,\n",
            "          label_smoothing_factor=0.0,\n",
            "          optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
            "          optim_args=None,\n",
            "          adafactor=False,\n",
            "          group_by_length=False,\n",
            "          length_column_name='length',\n",
            "          report_to=['tensorboard'],\n",
            "          ddp_find_unused_parameters=None,\n",
            "          ddp_bucket_cap_mb=None,\n",
            "          ddp_broadcast_buffers=None,\n",
            "          dataloader_pin_memory=True,\n",
            "          dataloader_persistent_workers=False,\n",
            "          skip_memory_metrics=True,\n",
            "          use_legacy_prediction_loop=False,\n",
            "          push_to_hub=False,\n",
            "          resume_from_checkpoint=None,\n",
            "          hub_model_id=None,\n",
            "          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
            "          hub_token=None,\n",
            "          hub_private_repo=False,\n",
            "          hub_always_push=False,\n",
            "          gradient_checkpointing=False,\n",
            "          gradient_checkpointing_kwargs={},\n",
            "          include_inputs_for_metrics=False,\n",
            "          eval_do_concat_batches=True,\n",
            "          fp16_backend='auto',\n",
            "          evaluation_strategy=None,\n",
            "          push_to_hub_model_id=None,\n",
            "          push_to_hub_organization=None,\n",
            "          push_to_hub_token=None,\n",
            "          mp_parameters='',\n",
            "          auto_find_batch_size=False,\n",
            "          full_determinism=False,\n",
            "          torchdynamo=None,\n",
            "          ray_scope='last',\n",
            "          ddp_timeout=1800,\n",
            "          torch_compile=False,\n",
            "          torch_compile_backend=None,\n",
            "          torch_compile_mode=None,\n",
            "          dispatch_batches=None,\n",
            "          split_batches=None,\n",
            "          include_tokens_per_second=False,\n",
            "          include_num_input_tokens_seen=False,\n",
            "          neftune_noise_alpha=None,\n",
            "          optim_target_modules=None,\n",
            "          batch_eval_metrics=False,\n",
            "          eval_on_start=False,\n",
            "          use_liger_kernel=False,\n",
            "          eval_use_gather_object=False,\n",
            "          dataset_text_field=None,\n",
            "          packing=False,\n",
            "          max_seq_length=None,\n",
            "          dataset_num_proc=None,\n",
            "          dataset_batch_size=1000,\n",
            "          model_init_kwargs=None,\n",
            "          dataset_kwargs=None,\n",
            "          eval_packing=None,\n",
            "          num_of_sequences=1024,\n",
            "          chars_per_token=3.6,\n",
            "          use_liger=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-01-30 09:45:01,249][oumi][rank0][pid:4366][MainThread][INFO]][device_utils.py:283] GPU Metrics Before Training: GPU runtime info: None.\n",
            "[2025-01-30 09:45:01,251][oumi][rank0][pid:4366][MainThread][INFO]][train.py:312] Training init time: 5.972s\n",
            "[2025-01-30 09:45:01,253][oumi][rank0][pid:4366][MainThread][INFO]][train.py:313] Starting training... (TrainerType.TRL_SFT, transformers: 4.45.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:06, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-01-30 09:45:09,276][oumi][rank0][pid:4366][MainThread][INFO]][train.py:320] Training is Complete.\n",
            "[2025-01-30 09:45:09,278][oumi][rank0][pid:4366][MainThread][INFO]][device_utils.py:283] GPU Metrics After Training: GPU runtime info: None.\n",
            "[2025-01-30 09:45:09,281][oumi][rank0][pid:4366][MainThread][INFO]][torch_utils.py:117] Peak GPU memory usage: 2.06 GB\n",
            "[2025-01-30 09:45:09,283][oumi][rank0][pid:4366][MainThread][INFO]][train.py:327] Saving final state...\n",
            "[2025-01-30 09:45:09,285][oumi][rank0][pid:4366][MainThread][INFO]][train.py:332] Saving final model...\n",
            "[2025-01-30 09:45:12,429][oumi][rank0][pid:4366][MainThread][INFO]][hf_trainer.py:102] Model has been saved at tour_tutorial/output\n",
            "[2025-01-30 09:45:12,432][oumi][rank0][pid:4366][MainThread][INFO]][train.py:339] \n",
            "\n",
            "¬ª We're always looking for feedback. What's one thing we can improve? https://oumi.ai/feedback\n"
          ]
        }
      ],
      "source": [
        "from oumi.core.configs import TrainingConfig\n",
        "from oumi.train import train\n",
        "\n",
        "config = TrainingConfig.from_yaml(str(Path(tutorial_dir) / \"train.yaml\"))\n",
        "\n",
        "train(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XfsWKEFSgtR"
      },
      "source": [
        "Congratulations, you've trained your first model using Oumi!\n",
        "\n",
        "You can also train your own custom Pytorch model. We cover that in depth in our [Finetuning Tutorial](https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Finetuning%20Tutorial.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdMUxYZcSgtR"
      },
      "source": [
        "# üß† Model Inference\n",
        "\n",
        "Now that you've trained a model, let's run inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z4bx6ibXSgtR"
      },
      "outputs": [],
      "source": [
        "yaml_content = f\"\"\"\n",
        "model:\n",
        "  model_name: \"{tutorial_dir}/output\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "\n",
        "generation:\n",
        "  max_new_tokens: 128\n",
        "  batch_size: 1\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{tutorial_dir}/infer.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XaxxRD1sSgtR",
        "outputId": "07db30cc-9f01-4198-be7c-a4938f096641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-01-30 09:45:28,411][oumi][rank0][pid:4366][MainThread][WARNING]][infer.py:19] No inference engine specified. Using the default 'native' engine.\n",
            "[2025-01-30 09:45:28,413][oumi][rank0][pid:4366][MainThread][INFO]][models.py:185] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-01-30 09:45:28,416][oumi][rank0][pid:4366][MainThread][INFO]][models.py:255] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
            "[2025-01-30 09:45:29,239][oumi][rank0][pid:4366][MainThread][INFO]][native_text_inference_engine.py:111] Setting EOS token id to `2`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conversation_id=None messages=[USER: Remember that we didn't train for long, so the results might not be great., ASSISTANT: I'm sorry for the inconvenience, but as a chatbot, I don't have the ability to access or process data from external sources. I'm designed to provide information and guidance based on the information I have available. I'm designed to be helpful and informative, but I don't have the capability to access or process data from external sources. If you have any specific questions or need help with a particular topic, feel free to ask.] metadata={}\n"
          ]
        }
      ],
      "source": [
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.infer import infer\n",
        "\n",
        "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"infer.yaml\"))\n",
        "\n",
        "input_text = (\n",
        "    \"Remember that we didn't train for long, so the results might not be great.\"\n",
        ")\n",
        "\n",
        "results = infer(config=config, inputs=[input_text])\n",
        "\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmlA3ynCSgtR"
      },
      "source": [
        "We can also run inference using the pretrained model by slightly tweaking our config:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Wbo2QAxoSgtR",
        "outputId": "2e6d8319-df7f-4a1e-b6f0-4f0d8aa3e5f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-01-30 09:45:46,767][oumi][rank0][pid:4366][MainThread][WARNING]][infer.py:19] No inference engine specified. Using the default 'native' engine.\n",
            "[2025-01-30 09:45:46,768][oumi][rank0][pid:4366][MainThread][INFO]][models.py:185] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-01-30 09:45:46,888][oumi][rank0][pid:4366][MainThread][INFO]][models.py:255] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
            "[2025-01-30 09:45:47,907][oumi][rank0][pid:4366][MainThread][INFO]][native_text_inference_engine.py:111] Setting EOS token id to `2`\n",
            "conversation_id=None messages=[USER: Input for the pretrained model: What is your name? , ASSISTANT: My name is Alex Chen. I'm a data scientist and AI assistant, trained on a vast dataset of text data, which I use to train my models for various tasks.] metadata={}\n"
          ]
        }
      ],
      "source": [
        "base_model_config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"infer.yaml\"))\n",
        "base_model_config.model.model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "\n",
        "input_text = \"Input for the pretrained model: What is your name? \"\n",
        "\n",
        "results = infer(config=base_model_config, inputs=[input_text])\n",
        "\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnxsJBCTSgtR"
      },
      "source": [
        "# üìä Evaluating a Model against Common Benchmarks\n",
        "\n",
        "You can use Oumi to evaluate pretrained and tuned models against standard benchmarks. For example, let's evaluate our tuned model against `Hellaswag`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tArOVDzVSgtR"
      },
      "outputs": [],
      "source": [
        "yaml_content = f\"\"\"\n",
        "model:\n",
        "  model_name: \"{tutorial_dir}/output\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "\n",
        "tasks:\n",
        "  - evaluation_platform: lm_harness\n",
        "    task_name: mmlu_college_computer_science\n",
        "\n",
        "generation:\n",
        "  batch_size: null # This will let LM HARNESS find the maximum possible batch size.\n",
        "output_dir: \"{tutorial_dir}/output/evaluation\"\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{tutorial_dir}/eval.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e-tcwtMhSgtR",
        "outputId": "b7ebc971-79a9-4dbd-edab-35d3dc37f89b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747,
          "referenced_widgets": [
            "a5ed2b93524a4824b031887bb2b7e6e7",
            "52bc43f0c65b49c1a662417308f75fd3",
            "916da4ded1f74a01955dc2a84c14fdb2",
            "ea831f9100464cd19589e3df95f0df50",
            "8ac54e299b594faf82ec201f0e3665ca",
            "c5e5d0e930994218b32a51fab7c441ce",
            "c74861dcf2e44973a5ab60aa83baeb66",
            "31241d717bbd4d76b59207114d89d2d7",
            "219b75688d5b47f5b7e7c015cce2e053",
            "1914b686875a499184f2d181fafc1128",
            "a5102d85f38b4c66809cbb60b0246840",
            "c274d615425b4887979bc45ac02e35c4",
            "be1a1b4437124b17a6e4248301b9c8a3",
            "88d8de79340644daa20b5aa3fcbf79a6",
            "51a70ca87d1e4a37b180e3b9e2851e23",
            "4da69529ba684be096e38ee719d0f6b1",
            "4ee881daa96d4d57ae3dfbdf849284b9",
            "0c17633b1cc64b32ba3610e8cd20ca5f",
            "0994fc32a12241f58d1047ec182bd267",
            "7d25ab2b87c84510bd01ac79105d6a61",
            "a3ffa2f745404be6b57ea0fa20a6568b",
            "79b3345b198f47f2b14fbd372de82186",
            "01a8edf4e0e24aca965b1d01b8e92abe",
            "53704d804b694c4faa1319fc14fd7b9c",
            "1595739fd39b4dd2aa614683f074b7f2",
            "82579b7410c84dba96b02711f325a6ea",
            "8384b86770964c18b5cd21111bd44fbc",
            "0be9579842e64655abea9580b743fa92",
            "26cac25d5eaf46d9a498f4a90e06dae7",
            "1cc1a1040ede4e1dafa5237b9207eab9",
            "c2edac0c91f2474f8a07aa8684b7c080",
            "7323519278d34511989fe3f58bfb2d7d",
            "6c5d8f078d33492ebd2adfb4943c1ee2",
            "7a85daa209f1455ca2db698e46c621e2",
            "d248b19336964290ab9d309c5f2a1bf0",
            "3546d4e5eb294832ba79f4023e45e959",
            "b2a31d655f4e4d1096908206f7a99eb5",
            "a494256615074d839066f4f05b3f31ca",
            "53ba36fd50884f53aee271470628be46",
            "22136090e98f4109843b4d33ae8ba36e",
            "5ad6e4f7c58147d4b0bfece5910093aa",
            "7bd73f941941476cb1ba6e0360625140",
            "b487dd57973a410faeb3979dde4051d2",
            "9479c5ab60c04ecb88de2788bafea7af",
            "5621f23e248d40e7b29bde1ebcdd2762",
            "56004600ee6e419f917c8be3eaaa837c",
            "7105b1e935764893b1f980310f0f2cb1",
            "a6cffaab66bf4f14ac4304d5b9c145f8",
            "b798b973a04b4b6dafae63e3eafd4846",
            "416687a2b7464bd19010d94ed07752c0",
            "366a775d5c0349fbbeebf9b70bedaa9c",
            "f736aaba7c8549d0bd6753a1653316a0",
            "db56b739b321404c995e60decfe6a091",
            "678cb21655a7442ca39c5a5c077cc4a4",
            "d1253e10f85b4f52aeaa0006fdc57656",
            "b4c5cca5c7f043b0ba4ca65dba8a8255",
            "2a7fa92c64a84d47ae76594f49447255",
            "b1a56e3a13d94e30b6e54e3ad64325a5",
            "d94be02da26a4eac88fc624000aee02b",
            "1ce88be009e14410bb3572bc9d4b4b35",
            "90cb6cc0f6424f0f9764f04ecf5ee3d3",
            "bf61432d942f4e7d876451521bae601e",
            "4036423dc7d840a7a980cc763373eb8a",
            "a1adcff8ff3444999550d213b46af7f5",
            "3bc4ff1bd6ac40cdb19b379b6258d854",
            "93fd704b32a04423b2dbdf2122729873"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-01-30 09:46:18,944][oumi][rank0][pid:4366][MainThread][INFO]][lm_harness.py:110] Starting evaluation...\n",
            "[2025-01-30 09:46:18,946][oumi][rank0][pid:4366][MainThread][INFO]][lm_harness.py:111] \tLM Harness `model_params`:\n",
            "{'device_map': 'auto',\n",
            " 'dtype': torch.bfloat16,\n",
            " 'parallelize': False,\n",
            " 'pretrained': 'tour_tutorial/output',\n",
            " 'trust_remote_code': False}\n",
            "[2025-01-30 09:46:18,948][oumi][rank0][pid:4366][MainThread][INFO]][lm_harness.py:112] \tLM Harness `task_params`:\n",
            "LMHarnessTaskParams(evaluation_platform='lm_harness',\n",
            "                    task_name='mmlu_college_computer_science',\n",
            "                    num_samples=None,\n",
            "                    eval_kwargs={},\n",
            "                    num_fewshot=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lm-eval:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm-eval:Initializing hf model, with arguments: {'pretrained': 'tour_tutorial/output', 'trust_remote_code': False, 'parallelize': False, 'dtype': torch.bfloat16, 'device_map': 'auto'}\n",
            "INFO:lm-eval:Using device 'cuda:0'\n",
            "INFO:lm-eval:Model parallel was set to False.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5ed2b93524a4824b031887bb2b7e6e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mmlu_no_train.py:   0%|          | 0.00/5.86k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c274d615425b4887979bc45ac02e35c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data.tar:   0%|          | 0.00/166M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01a8edf4e0e24aca965b1d01b8e92abe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a85daa209f1455ca2db698e46c621e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5621f23e248d40e7b29bde1ebcdd2762"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating dev split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4c5cca5c7f043b0ba4ca65dba8a8255"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lm-eval:Building contexts for mmlu_college_computer_science on rank 0...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 642.71it/s]\n",
            "INFO:lm-eval:Running loglikelihood requests\n",
            "Running loglikelihood requests:   0%|          | 0/400 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed argument batch_size = auto:1. Detecting largest batch size\n",
            "Determined largest batch size: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:12<00:00, 30.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-01-30 09:46:47,773][oumi][rank0][pid:4366][MainThread][INFO]][lm_harness.py:132] mmlu_college_computer_science's metric dict is {'acc,none': 0.26,\n",
            " 'acc_stderr,none': 0.044084400227680794,\n",
            " 'alias': 'college_computer_science'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'results': {'mmlu_college_computer_science': {'alias': 'college_computer_science',\n",
              "    'acc,none': 0.26,\n",
              "    'acc_stderr,none': 0.044084400227680794}}}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from oumi.core.configs import EvaluationConfig\n",
        "from oumi.evaluate import evaluate\n",
        "\n",
        "eval_config = EvaluationConfig.from_yaml(str(Path(tutorial_dir) / \"eval.yaml\"))\n",
        "\n",
        "# Uncomment the following line to run evals against the V1 HuggingFace Leaderboard.\n",
        "# This may take a while.\n",
        "# eval_config.data.datasets[0].dataset_name = \"huggingface_leaderboard_v1\"\n",
        "\n",
        "evaluate(eval_config)"
      ]
    },
    {
      "source": [
        "!pip install --upgrade Pillow torchvision"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gOtb5V6BlhIk",
        "outputId": "2d40e7f7-413d-48cf-c25b-666d014d3b19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (10.3.0)\n",
            "Collecting Pillow\n",
            "  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.19.1)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Collecting torch==2.6.0 (from torchvision)\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2024.9.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch==2.6.0->torchvision)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 10.3.0\n",
            "    Uninstalling pillow-10.3.0:\n",
            "      Successfully uninstalled pillow-10.3.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1\n",
            "    Uninstalling torch-2.4.1:\n",
            "      Successfully uninstalled torch-2.4.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.1\n",
            "    Uninstalling torchvision-0.19.1:\n",
            "      Successfully uninstalled torchvision-0.19.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "oumi 0.1.3 requires pillow<10.4,>=10.3.0, but you have pillow 11.1.0 which is incompatible.\n",
            "oumi 0.1.3 requires torch<2.5.0,>=2.4.0, but you have torch 2.6.0 which is incompatible.\n",
            "oumi 0.1.3 requires torchvision<0.20,>=0.19.0, but you have torchvision 0.21.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-11.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.6.0 torchvision-0.21.0 triton-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "torch",
                  "torchgen",
                  "triton"
                ]
              },
              "id": "9b4a91dacfc44450a0d8664db15e3637"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUnIyaZfSgtS"
      },
      "source": [
        "# ‚òÅÔ∏è Launching Jobs\n",
        "\n",
        "Oftentimes you'll need to run various tasks (training, evaluation, etc.) on remote hardware that's better suited for the task. Oumi can handle this for you by launching jobs on various compute clusters. For more information about running jobs, see our [Running Jobs Remotely tutorial](https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Running%20Jobs%20Remotely.ipynb). For running jobs on custom clusters, see our [Launching Jobs on Custom Clusters tutorial](https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Launching%20Jobs%20on%20Custom%20Clusters.ipynb).\n",
        "\n",
        "\n",
        "Today, Oumi supports running jobs on several cloud provider platforms.\n",
        "\n",
        "For the latest list, we can run the `which_clouds` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MJipzQl4SgtS",
        "outputId": "809478ce-6f69-4dc6-d76f-1627b4bc9307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supported Clouds in Oumi:\n",
            "local\n",
            "polaris\n",
            "runpod\n",
            "gcp\n",
            "lambda\n",
            "aws\n",
            "azure\n"
          ]
        }
      ],
      "source": [
        "import oumi.launcher as launcher\n",
        "\n",
        "print(\"Supported Clouds in Oumi:\")\n",
        "for cloud in launcher.which_clouds():\n",
        "    print(cloud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VdZqPtpSgtS"
      },
      "source": [
        "Let's run a simple \"Hello World\" job locally to demonstrate how to use the Oumi job launcher. This job will echo `Hello World`, then run the same training job executed above. Running this job on a cloud provider like GCP simply involves changing the `cloud` field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "15_fKEiNSgtS"
      },
      "outputs": [],
      "source": [
        "yaml_content = f\"\"\"\n",
        "name: hello-world\n",
        "resources:\n",
        "  cloud: local\n",
        "\n",
        "working_dir: .\n",
        "\n",
        "envs:\n",
        "  TEST_ENV_VARIABLE: '\"Hello, World!\"'\n",
        "  OUMI_LOGGING_DIR: \"{tutorial_dir}/logs\"\n",
        "\n",
        "run: |\n",
        "  echo \"$TEST_ENV_VARIABLE\"\n",
        "  oumi train -c {tutorial_dir}/train.yaml\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{tutorial_dir}/job.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VA-n5iUASgtS",
        "outputId": "e12a2fdb-8fa9-4d48-df67-6b3e8e5457ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job is running...\n",
            "Job is running...\n",
            "Job is running...\n",
            "Job is done!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "job_config = launcher.JobConfig.from_yaml(str(Path(tutorial_dir) / \"job.yaml\"))\n",
        "cluster, job_status = launcher.up(job_config, cluster_name=None)\n",
        "\n",
        "while job_status and not job_status.done:\n",
        "    print(\"Job is running...\")\n",
        "    time.sleep(15)\n",
        "    job_status = cluster.get_job(job_status.id)\n",
        "print(\"Job is done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuVVlpbOSgtS"
      },
      "source": [
        "The job created logs under our tutorial directory. Let's take a look at the directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-BGuSP6XTVKh",
        "outputId": "59949870-687f-4bdd-8846-07f445393d90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2025_01_30_09_52_19_598_0.stdout', '2025_01_30_09_52_19_598_0.stderr']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "logs_dir = f\"{tutorial_dir}/logs\"\n",
        "os.listdir(logs_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_iz4i4ATXuB"
      },
      "source": [
        "Now let's parse the logfiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RbhUaUchSgtT",
        "outputId": "e12137d3-6cd7-49b1-c08c-d2148d06f464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log file: tour_tutorial/logs/2025_01_30_09_52_19_598_0.stdout\n",
            "\"Hello, World!\"\n",
            "\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "@                 @\n",
            "@   @@@@@  @  @   @\n",
            "@   @   @  @  @   @\n",
            "@   @@@@@  @@@@   @\n",
            "@                 @\n",
            "@   @@@@@@@   @   @\n",
            "@   @  @  @   @   @\n",
            "@   @  @  @   @   @\n",
            "@                 @\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "\n",
            "[2025-01-30 09:52:27,261][oumi][rank0][pid:6786][MainThread][INFO]][distributed.py:546] Setting random seed to 42 on rank 0.\n",
            "[2025-01-30 09:52:31,697][oumi][rank0][pid:6786][MainThread][INFO]][torch_utils.py:66] Torch version: 2.4.1+cu121. NumPy version: 1.26.4\n",
            "[2025-01-30 09:52:31,699][oumi][rank0][pid:6786][MainThread][INFO]][torch_utils.py:72] CUDA version: 12.1 CuDNN version: 90.1.0\n",
            "[2025-01-30 09:52:31,710][oumi][rank0][pid:6786][MainThread][INFO]][torch_utils.py:106] CPU cores: 2 CUDA devices: 1\n",
            "device(0)='Tesla T4' Capability: (7, 5) Memory: [Total: 14.74GiB Free: 6.17GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
            "[2025-01-30 09:52:31,712][oumi][rank0][pid:6786][MainThread][INFO]][train.py:133] Oumi version: 0.1.3\n",
            "[2025-01-30 09:52:31,715][oumi][rank0][pid:6786][MainThread][INFO]][train.py:174] TrainingConfig:\n",
            "TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='yahma/alpaca-cleaned',\n",
            "                                                                                dataset_path=None,\n",
            "                                                                                subset=None,\n",
            "                                                                                split='train',\n",
            "                                                                                dataset_kwargs={},\n",
            "                                                                                sample_count=None,\n",
            "                                                                                mixture_proportion=None,\n",
            "                                                                                shuffle=False,\n",
            "                                                                                seed=None,\n",
            "                                                                                shuffle_buffer_size=1000,\n",
            "                                                                                trust_remote_code=False,\n",
            "                                                                                transform_num_workers=None)],\n",
            "                                                        collator_name=None,\n",
            "                                                        pack=False,\n",
            "                                                        stream=False,\n",
            "                                                        target_col='prompt',\n",
            "                                                        mixture_strategy='first_exhausted',\n",
            "                                                        seed=None,\n",
            "                                                        use_async_dataset=False,\n",
            "                                                        use_torchdata=None),\n",
            "                               test=DatasetSplitParams(datasets=[],\n",
            "                                                       collator_name=None,\n",
            "                                                       pack=False,\n",
            "                                                       stream=False,\n",
            "                                                       target_col=None,\n",
            "                                                       mixture_strategy='first_exhausted',\n",
            "                                                       seed=None,\n",
            "                                                       use_async_dataset=False,\n",
            "                                                       use_torchdata=None),\n",
            "                               validation=DatasetSplitParams(datasets=[],\n",
            "                                                             collator_name=None,\n",
            "                                                             pack=False,\n",
            "                                                             stream=False,\n",
            "                                                             target_col=None,\n",
            "                                                             mixture_strategy='first_exhausted',\n",
            "                                                             seed=None,\n",
            "                                                             use_async_dataset=False,\n",
            "                                                             use_torchdata=None)),\n",
            "               model=ModelParams(model_name='HuggingFaceTB/SmolLM2-135M-Instruct',\n",
            "                                 adapter_model=None,\n",
            "                                 tokenizer_name=None,\n",
            "                                 tokenizer_pad_token=None,\n",
            "                                 tokenizer_kwargs={},\n",
            "                                 model_max_length=None,\n",
            "                                 load_pretrained_weights=True,\n",
            "                                 trust_remote_code=True,\n",
            "                                 torch_dtype_str='bfloat16',\n",
            "                                 compile=False,\n",
            "                                 chat_template=None,\n",
            "                                 attn_implementation=None,\n",
            "                                 device_map='auto',\n",
            "                                 model_kwargs={},\n",
            "                                 enable_liger_kernel=False,\n",
            "                                 shard_for_eval=False,\n",
            "                                 freeze_layers=[]),\n",
            "               training=TrainingParams(use_peft=False,\n",
            "                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,\n",
            "                                       enable_gradient_checkpointing=False,\n",
            "                                       gradient_checkpointing_kwargs={},\n",
            "                                       output_dir='tour_tutorial/output',\n",
            "                                       per_device_train_batch_size=2,\n",
            "                                       per_device_eval_batch_size=8,\n",
            "                                       gradient_accumulation_steps=1,\n",
            "                                       max_steps=10,\n",
            "                                       num_train_epochs=3,\n",
            "                                       save_epoch=False,\n",
            "                                       save_steps=500,\n",
            "                                       save_final_model=True,\n",
            "                                       seed=42,\n",
            "                                       run_name='smollm2_135m_sft',\n",
            "                                       metrics_function=None,\n",
            "                                       log_level='info',\n",
            "                                       dep_log_level='warning',\n",
            "                                       enable_wandb=False,\n",
            "                                       enable_tensorboard=True,\n",
            "                                       logging_strategy='steps',\n",
            "                                       logging_dir=None,\n",
            "                                       logging_steps=50,\n",
            "                                       logging_first_step=False,\n",
            "                                       eval_strategy='no',\n",
            "                                       eval_steps=500,\n",
            "                                       learning_rate=5e-05,\n",
            "                                       lr_scheduler_type='linear',\n",
            "                                       lr_scheduler_kwargs={},\n",
            "                                       warmup_ratio=None,\n",
            "                                       warmup_steps=None,\n",
            "                                       optimizer='adamw_torch',\n",
            "                                       weight_decay=0.0,\n",
            "                                       adam_beta1=0.9,\n",
            "                                       adam_beta2=0.999,\n",
            "                                       adam_epsilon=1e-08,\n",
            "                                       sgd_momentum=0.0,\n",
            "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
            "                                       compile=False,\n",
            "                                       include_performance_metrics=False,\n",
            "                                       include_alternative_mfu_metrics=False,\n",
            "                                       log_model_summary=False,\n",
            "                                       resume_from_checkpoint=None,\n",
            "                                       try_resume_from_last_checkpoint=False,\n",
            "                                       dataloader_num_workers=0,\n",
            "                                       dataloader_prefetch_factor=None,\n",
            "                                       dataloader_main_process_only=None,\n",
            "                                       ddp_find_unused_parameters=None,\n",
            "                                       max_grad_norm=1.0,\n",
            "                                       trainer_kwargs={},\n",
            "                                       profiler=ProfilerParams(save_dir=None,\n",
            "                                                               enable_cpu_profiling=False,\n",
            "                                                               enable_cuda_profiling=False,\n",
            "                                                               record_shapes=False,\n",
            "                                                               profile_memory=False,\n",
            "                                                               with_stack=False,\n",
            "                                                               with_flops=False,\n",
            "                                                               with_modules=False,\n",
            "                                                               row_limit=50,\n",
            "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
            "                                                                                               wait=0,\n",
            "                                                                                               warmup=1,\n",
            "                                                                                               active=3,\n",
            "                                                                                               repeat=1,\n",
            "                                                                                               skip_first=1)),\n",
            "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
            "                                                                 collect_telemetry_for_all_ranks=False,\n",
            "                                                                 track_gpu_temperature=False),\n",
            "                                       empty_device_cache_steps=None,\n",
            "                                       nccl_default_timeout_minutes=None),\n",
            "               peft=PeftParams(lora_r=8,\n",
            "                               lora_alpha=8,\n",
            "                               lora_dropout=0.0,\n",
            "                               lora_target_modules=None,\n",
            "                               lora_modules_to_save=None,\n",
            "                               lora_bias='none',\n",
            "                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,\n",
            "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
            "                               q_lora=False,\n",
            "                               q_lora_bits=4,\n",
            "                               bnb_4bit_quant_type='fp4',\n",
            "                               use_bnb_nested_quant=False,\n",
            "                               bnb_4bit_quant_storage='uint8',\n",
            "                               bnb_4bit_compute_dtype='float32',\n",
            "                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),\n",
            "               fsdp=FSDPParams(enable_fsdp=False,\n",
            "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
            "                               cpu_offload=False,\n",
            "                               mixed_precision=None,\n",
            "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
            "                               forward_prefetch=False,\n",
            "                               use_orig_params=None,\n",
            "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
            "                               auto_wrap_policy=<AutoWrapPolicy.NO_WRAP: 'NO_WRAP'>,\n",
            "                               min_num_params=100000,\n",
            "                               transformer_layer_cls=None,\n",
            "                               sync_module_states=True))\n",
            "[2025-01-30 09:52:32,083][oumi][rank0][pid:6786][MainThread][INFO]][models.py:185] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-01-30 09:52:32,083][oumi][rank0][pid:6786][MainThread][INFO]][models.py:255] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
            "[2025-01-30 09:52:32,814][oumi][rank0][pid:6786][MainThread][INFO]][base_map_dataset.py:68] Creating map dataset (type: AlpacaDataset) dataset_name: 'yahma/alpaca-cleaned', dataset_path: 'None'...\n",
            "[2025-01-30 09:52:35,174][oumi][rank0][pid:6786][MainThread][INFO]][base_map_dataset.py:472] Dataset Info:\n",
            "\tSplit: train\n",
            "\tVersion: 0.0.0\n",
            "\tDataset size: 40283906\n",
            "\tDownload size: 44307561\n",
            "\tSize: 84591467 bytes\n",
            "\tRows: 51760\n",
            "\tColumns: ['output', 'input', 'instruction']\n",
            "[2025-01-30 09:52:35,695][oumi][rank0][pid:6786][MainThread][INFO]][base_map_dataset.py:411] Loaded DataFrame with shape: (51760, 3). Columns:\n",
            "output         object\n",
            "input          object\n",
            "instruction    object\n",
            "dtype: object\n",
            "[2025-01-30 09:52:35,743][oumi][rank0][pid:6786][MainThread][INFO]][base_map_dataset.py:297] AlpacaDataset: features=dict_keys(['input_ids', 'attention_mask'])\n",
            "[2025-01-30 09:52:36,355][oumi][rank0][pid:6786][MainThread][INFO]][base_map_dataset.py:361] Finished transforming dataset (AlpacaDataset)! Speed: 84630.42 examples/sec. Examples: 51760. Duration: 0.6 sec. Transform workers: 1.\n",
            "[2025-01-30 09:52:36,778][oumi][rank0][pid:6786][MainThread][INFO]][torch_profiler_utils.py:150] PROF: Torch Profiler disabled!\n",
            "[2025-01-30 09:52:36,803][oumi][rank0][pid:6786][MainThread][INFO]][training.py:49] SFTConfig(output_dir='tour_tutorial/output',\n",
            "          overwrite_output_dir=False,\n",
            "          do_train=False,\n",
            "          do_eval=False,\n",
            "          do_predict=False,\n",
            "          eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
            "          prediction_loss_only=False,\n",
            "          per_device_train_batch_size=2,\n",
            "          per_device_eval_batch_size=8,\n",
            "          per_gpu_train_batch_size=None,\n",
            "          per_gpu_eval_batch_size=None,\n",
            "          gradient_accumulation_steps=1,\n",
            "          eval_accumulation_steps=None,\n",
            "          eval_delay=0,\n",
            "          torch_empty_cache_steps=None,\n",
            "          learning_rate=5e-05,\n",
            "          weight_decay=0.0,\n",
            "          adam_beta1=0.9,\n",
            "          adam_beta2=0.999,\n",
            "          adam_epsilon=1e-08,\n",
            "          max_grad_norm=1.0,\n",
            "          num_train_epochs=3,\n",
            "          max_steps=10,\n",
            "          lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>,\n",
            "          lr_scheduler_kwargs={},\n",
            "          warmup_ratio=0.0,\n",
            "          warmup_steps=0,\n",
            "          log_level='warning',\n",
            "          log_level_replica='warning',\n",
            "          log_on_each_node=True,\n",
            "          logging_dir='tour_tutorial/output/runs/Jan30_09-52-36_fc2e07b569e0',\n",
            "          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
            "          logging_first_step=False,\n",
            "          logging_steps=50,\n",
            "          logging_nan_inf_filter=True,\n",
            "          save_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
            "          save_steps=500,\n",
            "          save_total_limit=None,\n",
            "          save_safetensors=True,\n",
            "          save_on_each_node=False,\n",
            "          save_only_model=False,\n",
            "          restore_callback_states_from_checkpoint=False,\n",
            "          no_cuda=False,\n",
            "          use_cpu=False,\n",
            "          use_mps_device=False,\n",
            "          seed=42,\n",
            "          data_seed=None,\n",
            "          jit_mode_eval=False,\n",
            "          use_ipex=False,\n",
            "          bf16=False,\n",
            "          fp16=False,\n",
            "          fp16_opt_level='O1',\n",
            "          half_precision_backend='auto',\n",
            "          bf16_full_eval=False,\n",
            "          fp16_full_eval=False,\n",
            "          tf32=None,\n",
            "          local_rank=0,\n",
            "          ddp_backend=None,\n",
            "          tpu_num_cores=None,\n",
            "          tpu_metrics_debug=False,\n",
            "          debug=[],\n",
            "          dataloader_drop_last=False,\n",
            "          eval_steps=500,\n",
            "          dataloader_num_workers=0,\n",
            "          dataloader_prefetch_factor=None,\n",
            "          past_index=-1,\n",
            "          run_name='smollm2_135m_sft',\n",
            "          disable_tqdm=False,\n",
            "          remove_unused_columns=True,\n",
            "          label_names=None,\n",
            "          load_best_model_at_end=False,\n",
            "          metric_for_best_model=None,\n",
            "          greater_is_better=None,\n",
            "          ignore_data_skip=False,\n",
            "          fsdp=[],\n",
            "          fsdp_min_num_params=0,\n",
            "          fsdp_config={'min_num_params': 0,\n",
            "                       'xla': False,\n",
            "                       'xla_fsdp_grad_ckpt': False,\n",
            "                       'xla_fsdp_v2': False},\n",
            "          fsdp_transformer_layer_cls_to_wrap=None,\n",
            "          accelerator_config=AcceleratorConfig(split_batches=False,\n",
            "                                               dispatch_batches=None,\n",
            "                                               even_batches=True,\n",
            "                                               use_seedable_sampler=True,\n",
            "                                               non_blocking=False,\n",
            "                                               gradient_accumulation_kwargs=None,\n",
            "                                               use_configured_state=False),\n",
            "          deepspeed=None,\n",
            "          label_smoothing_factor=0.0,\n",
            "          optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>,\n",
            "          optim_args=None,\n",
            "          adafactor=False,\n",
            "          group_by_length=False,\n",
            "          length_column_name='length',\n",
            "          report_to=['tensorboard'],\n",
            "          ddp_find_unused_parameters=None,\n",
            "          ddp_bucket_cap_mb=None,\n",
            "          ddp_broadcast_buffers=None,\n",
            "          dataloader_pin_memory=True,\n",
            "          dataloader_persistent_workers=False,\n",
            "          skip_memory_metrics=True,\n",
            "          use_legacy_prediction_loop=False,\n",
            "          push_to_hub=False,\n",
            "          resume_from_checkpoint=None,\n",
            "          hub_model_id=None,\n",
            "          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
            "          hub_token=None,\n",
            "          hub_private_repo=False,\n",
            "          hub_always_push=False,\n",
            "          gradient_checkpointing=False,\n",
            "          gradient_checkpointing_kwargs={},\n",
            "          include_inputs_for_metrics=False,\n",
            "          eval_do_concat_batches=True,\n",
            "          fp16_backend='auto',\n",
            "          evaluation_strategy=None,\n",
            "          push_to_hub_model_id=None,\n",
            "          push_to_hub_organization=None,\n",
            "          push_to_hub_token=None,\n",
            "          mp_parameters='',\n",
            "          auto_find_batch_size=False,\n",
            "          full_determinism=False,\n",
            "          torchdynamo=None,\n",
            "          ray_scope='last',\n",
            "          ddp_timeout=1800,\n",
            "          torch_compile=False,\n",
            "          torch_compile_backend=None,\n",
            "          torch_compile_mode=None,\n",
            "          dispatch_batches=None,\n",
            "          split_batches=None,\n",
            "          include_tokens_per_second=False,\n",
            "          include_num_input_tokens_seen=False,\n",
            "          neftune_noise_alpha=None,\n",
            "          optim_target_modules=None,\n",
            "          batch_eval_metrics=False,\n",
            "          eval_on_start=False,\n",
            "          use_liger_kernel=False,\n",
            "          eval_use_gather_object=False,\n",
            "          dataset_text_field=None,\n",
            "          packing=False,\n",
            "          max_seq_length=None,\n",
            "          dataset_num_proc=None,\n",
            "          dataset_batch_size=1000,\n",
            "          model_init_kwargs=None,\n",
            "          dataset_kwargs=None,\n",
            "          eval_packing=None,\n",
            "          num_of_sequences=1024,\n",
            "          chars_per_token=3.6,\n",
            "          use_liger=False)\n",
            "[2025-01-30 09:52:36,820][oumi][rank0][pid:6786][MainThread][INFO]][device_utils.py:283] GPU Metrics Before Training: GPU runtime info: None.\n",
            "[2025-01-30 09:52:36,820][oumi][rank0][pid:6786][MainThread][INFO]][train.py:312] Training init time: 5.123s\n",
            "[2025-01-30 09:52:36,820][oumi][rank0][pid:6786][MainThread][INFO]][train.py:313] Starting training... (TrainerType.TRL_SFT, transformers: 4.45.2)\n",
            "{'train_runtime': 7.8347, 'train_samples_per_second': 2.553, 'train_steps_per_second': 1.276, 'train_loss': 2.03830509185791, 'epoch': 0.0}\n",
            "[2025-01-30 09:52:45,059][oumi][rank0][pid:6786][MainThread][INFO]][train.py:320] Training is Complete.\n",
            "[2025-01-30 09:52:45,059][oumi][rank0][pid:6786][MainThread][INFO]][device_utils.py:283] GPU Metrics After Training: GPU runtime info: None.\n",
            "[2025-01-30 09:52:45,059][oumi][rank0][pid:6786][MainThread][INFO]][torch_utils.py:117] Peak GPU memory usage: 1.86 GB\n",
            "[2025-01-30 09:52:45,059][oumi][rank0][pid:6786][MainThread][INFO]][train.py:327] Saving final state...\n",
            "[2025-01-30 09:52:45,059][oumi][rank0][pid:6786][MainThread][INFO]][train.py:332] Saving final model...\n",
            "[2025-01-30 09:52:46,880][oumi][rank0][pid:6786][MainThread][INFO]][hf_trainer.py:102] Model has been saved at tour_tutorial/output\n",
            "[2025-01-30 09:52:46,880][oumi][rank0][pid:6786][MainThread][INFO]][train.py:339] \n",
            "\n",
            "¬ª We're always looking for feedback. What's one thing we can improve? https://oumi.ai/feedback\n",
            "\n",
            "Log file: tour_tutorial/logs/2025_01_30_09_52_19_598_0.stderr\n",
            "2025-01-30 09:52:28.949217: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738230748.971918    6786 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738230748.979001    6786 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\n",
            " 10%|‚ñà         | 1/10 [00:01<00:11,  1.23s/it]\n",
            " 20%|‚ñà‚ñà        | 2/10 [00:01<00:04,  1.61it/s]\n",
            " 30%|‚ñà‚ñà‚ñà       | 3/10 [00:01<00:02,  2.38it/s]\n",
            " 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:01<00:02,  2.82it/s]\n",
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:02<00:01,  3.42it/s]\n",
            " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:02<00:01,  3.58it/s]\n",
            " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:02<00:00,  4.10it/s]\n",
            " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:02<00:00,  3.75it/s]\n",
            " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:03<00:00,  3.94it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  3.97it/s]\n",
            "                                               \n",
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:07<00:00,  3.97it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:07<00:00,  1.28it/s]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for log_file in Path(logs_dir).iterdir():\n",
        "    print(f\"Log file: {log_file}\")\n",
        "    with open(log_file) as f:\n",
        "        print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnWJ_XgiSgtT"
      },
      "source": [
        "# ‚öôÔ∏è Customizing Datasets and Clusters\n",
        "\n",
        "Oumi offers rich customization that allows users to build custom solutions on top of our existing building blocks. Several of Oumi's primary resources (Datasets, Clouds, etc.) leverage the Oumi Registry when invoked.\n",
        "\n",
        "This registry allows users to build custom classes that function as drop-in replacements for core functionality.\n",
        "\n",
        "For more details on registering custom datasets, see the [tutorial here](https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Datasets%20Tutorial.ipynb).\n",
        "\n",
        "For a tutorial on writing a custom cloud/cluster for running jobs, see the [tutorial here](https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Launching%20Jobs%20on%20Custom%20Clusters.ipynb).\n",
        "\n",
        "You can find further information about the required registry decorators [here](https://oumi.ai/docs/en/latest/api/oumi.core.registry.html#oumi.core.registry.register_cloud_builder)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcM7KZ2OSgtT"
      },
      "source": [
        "# üß≠ What's Next?\n",
        "\n",
        "Now that you've completed the basic tour, you're ready to tackle the other [notebook guides & tutorials](https://oumi.ai/docs/en/latest/get_started/tutorials.html).\n",
        "\n",
        "If you have not already, make sure to take a look at the [Quickstart](https://oumi.ai/docs/en/latest/get_started/quickstart.html) for an overview of our CLI."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5ed2b93524a4824b031887bb2b7e6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52bc43f0c65b49c1a662417308f75fd3",
              "IPY_MODEL_916da4ded1f74a01955dc2a84c14fdb2",
              "IPY_MODEL_ea831f9100464cd19589e3df95f0df50"
            ],
            "layout": "IPY_MODEL_8ac54e299b594faf82ec201f0e3665ca"
          }
        },
        "52bc43f0c65b49c1a662417308f75fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5e5d0e930994218b32a51fab7c441ce",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c74861dcf2e44973a5ab60aa83baeb66",
            "value": "README.md:‚Äá100%"
          }
        },
        "916da4ded1f74a01955dc2a84c14fdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31241d717bbd4d76b59207114d89d2d7",
            "max": 1115,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_219b75688d5b47f5b7e7c015cce2e053",
            "value": 1115
          }
        },
        "ea831f9100464cd19589e3df95f0df50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1914b686875a499184f2d181fafc1128",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a5102d85f38b4c66809cbb60b0246840",
            "value": "‚Äá1.11k/1.11k‚Äá[00:00&lt;00:00,‚Äá80.6kB/s]"
          }
        },
        "8ac54e299b594faf82ec201f0e3665ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5e5d0e930994218b32a51fab7c441ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c74861dcf2e44973a5ab60aa83baeb66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31241d717bbd4d76b59207114d89d2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "219b75688d5b47f5b7e7c015cce2e053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1914b686875a499184f2d181fafc1128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5102d85f38b4c66809cbb60b0246840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c274d615425b4887979bc45ac02e35c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be1a1b4437124b17a6e4248301b9c8a3",
              "IPY_MODEL_88d8de79340644daa20b5aa3fcbf79a6",
              "IPY_MODEL_51a70ca87d1e4a37b180e3b9e2851e23"
            ],
            "layout": "IPY_MODEL_4da69529ba684be096e38ee719d0f6b1"
          }
        },
        "be1a1b4437124b17a6e4248301b9c8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ee881daa96d4d57ae3dfbdf849284b9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0c17633b1cc64b32ba3610e8cd20ca5f",
            "value": "mmlu_no_train.py:‚Äá100%"
          }
        },
        "88d8de79340644daa20b5aa3fcbf79a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0994fc32a12241f58d1047ec182bd267",
            "max": 5857,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d25ab2b87c84510bd01ac79105d6a61",
            "value": 5857
          }
        },
        "51a70ca87d1e4a37b180e3b9e2851e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ffa2f745404be6b57ea0fa20a6568b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_79b3345b198f47f2b14fbd372de82186",
            "value": "‚Äá5.86k/5.86k‚Äá[00:00&lt;00:00,‚Äá481kB/s]"
          }
        },
        "4da69529ba684be096e38ee719d0f6b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ee881daa96d4d57ae3dfbdf849284b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c17633b1cc64b32ba3610e8cd20ca5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0994fc32a12241f58d1047ec182bd267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d25ab2b87c84510bd01ac79105d6a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3ffa2f745404be6b57ea0fa20a6568b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79b3345b198f47f2b14fbd372de82186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01a8edf4e0e24aca965b1d01b8e92abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53704d804b694c4faa1319fc14fd7b9c",
              "IPY_MODEL_1595739fd39b4dd2aa614683f074b7f2",
              "IPY_MODEL_82579b7410c84dba96b02711f325a6ea"
            ],
            "layout": "IPY_MODEL_8384b86770964c18b5cd21111bd44fbc"
          }
        },
        "53704d804b694c4faa1319fc14fd7b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0be9579842e64655abea9580b743fa92",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_26cac25d5eaf46d9a498f4a90e06dae7",
            "value": "data.tar:‚Äá100%"
          }
        },
        "1595739fd39b4dd2aa614683f074b7f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cc1a1040ede4e1dafa5237b9207eab9",
            "max": 166184960,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2edac0c91f2474f8a07aa8684b7c080",
            "value": 166184960
          }
        },
        "82579b7410c84dba96b02711f325a6ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7323519278d34511989fe3f58bfb2d7d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6c5d8f078d33492ebd2adfb4943c1ee2",
            "value": "‚Äá166M/166M‚Äá[00:00&lt;00:00,‚Äá209MB/s]"
          }
        },
        "8384b86770964c18b5cd21111bd44fbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0be9579842e64655abea9580b743fa92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26cac25d5eaf46d9a498f4a90e06dae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cc1a1040ede4e1dafa5237b9207eab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2edac0c91f2474f8a07aa8684b7c080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7323519278d34511989fe3f58bfb2d7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c5d8f078d33492ebd2adfb4943c1ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a85daa209f1455ca2db698e46c621e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d248b19336964290ab9d309c5f2a1bf0",
              "IPY_MODEL_3546d4e5eb294832ba79f4023e45e959",
              "IPY_MODEL_b2a31d655f4e4d1096908206f7a99eb5"
            ],
            "layout": "IPY_MODEL_a494256615074d839066f4f05b3f31ca"
          }
        },
        "d248b19336964290ab9d309c5f2a1bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53ba36fd50884f53aee271470628be46",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_22136090e98f4109843b4d33ae8ba36e",
            "value": "Generating‚Äátest‚Äásplit:‚Äá"
          }
        },
        "3546d4e5eb294832ba79f4023e45e959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ad6e4f7c58147d4b0bfece5910093aa",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7bd73f941941476cb1ba6e0360625140",
            "value": 1
          }
        },
        "b2a31d655f4e4d1096908206f7a99eb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b487dd57973a410faeb3979dde4051d2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9479c5ab60c04ecb88de2788bafea7af",
            "value": "‚Äá100/0‚Äá[00:00&lt;00:00,‚Äá‚Äá7.87‚Äáexamples/s]"
          }
        },
        "a494256615074d839066f4f05b3f31ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53ba36fd50884f53aee271470628be46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22136090e98f4109843b4d33ae8ba36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ad6e4f7c58147d4b0bfece5910093aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7bd73f941941476cb1ba6e0360625140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b487dd57973a410faeb3979dde4051d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9479c5ab60c04ecb88de2788bafea7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5621f23e248d40e7b29bde1ebcdd2762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56004600ee6e419f917c8be3eaaa837c",
              "IPY_MODEL_7105b1e935764893b1f980310f0f2cb1",
              "IPY_MODEL_a6cffaab66bf4f14ac4304d5b9c145f8"
            ],
            "layout": "IPY_MODEL_b798b973a04b4b6dafae63e3eafd4846"
          }
        },
        "56004600ee6e419f917c8be3eaaa837c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_416687a2b7464bd19010d94ed07752c0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_366a775d5c0349fbbeebf9b70bedaa9c",
            "value": "Generating‚Äávalidation‚Äásplit:‚Äá"
          }
        },
        "7105b1e935764893b1f980310f0f2cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f736aaba7c8549d0bd6753a1653316a0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db56b739b321404c995e60decfe6a091",
            "value": 1
          }
        },
        "a6cffaab66bf4f14ac4304d5b9c145f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_678cb21655a7442ca39c5a5c077cc4a4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d1253e10f85b4f52aeaa0006fdc57656",
            "value": "‚Äá11/0‚Äá[00:00&lt;00:00,‚Äá401.63‚Äáexamples/s]"
          }
        },
        "b798b973a04b4b6dafae63e3eafd4846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "416687a2b7464bd19010d94ed07752c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "366a775d5c0349fbbeebf9b70bedaa9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f736aaba7c8549d0bd6753a1653316a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "db56b739b321404c995e60decfe6a091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "678cb21655a7442ca39c5a5c077cc4a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1253e10f85b4f52aeaa0006fdc57656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4c5cca5c7f043b0ba4ca65dba8a8255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a7fa92c64a84d47ae76594f49447255",
              "IPY_MODEL_b1a56e3a13d94e30b6e54e3ad64325a5",
              "IPY_MODEL_d94be02da26a4eac88fc624000aee02b"
            ],
            "layout": "IPY_MODEL_1ce88be009e14410bb3572bc9d4b4b35"
          }
        },
        "2a7fa92c64a84d47ae76594f49447255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90cb6cc0f6424f0f9764f04ecf5ee3d3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bf61432d942f4e7d876451521bae601e",
            "value": "Generating‚Äádev‚Äásplit:‚Äá"
          }
        },
        "b1a56e3a13d94e30b6e54e3ad64325a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4036423dc7d840a7a980cc763373eb8a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1adcff8ff3444999550d213b46af7f5",
            "value": 1
          }
        },
        "d94be02da26a4eac88fc624000aee02b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bc4ff1bd6ac40cdb19b379b6258d854",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_93fd704b32a04423b2dbdf2122729873",
            "value": "‚Äá5/0‚Äá[00:00&lt;00:00,‚Äá‚Äá6.71‚Äáexamples/s]"
          }
        },
        "1ce88be009e14410bb3572bc9d4b4b35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90cb6cc0f6424f0f9764f04ecf5ee3d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf61432d942f4e7d876451521bae601e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4036423dc7d840a7a980cc763373eb8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a1adcff8ff3444999550d213b46af7f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3bc4ff1bd6ac40cdb19b379b6258d854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93fd704b32a04423b2dbdf2122729873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}