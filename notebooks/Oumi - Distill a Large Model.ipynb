{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kxExmfV_k_h"
      },
      "source": [
        "<div class=\"align-center\">\n",
        "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
        "\n",
        "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
        "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
        "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
        "</div>\n",
        "\n",
        "üëã Welcome to Open Universal Machine Intelligence (Oumi)!\n",
        "\n",
        "üöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](hhttps://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
        "\n",
        "ü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
        "\n",
        "‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk7zb0Pi_k_j"
      },
      "source": [
        "# Distillation Overview\n",
        "\n",
        "In this tutorial, we'll fine-tune a small language model (SLM) from the outputs of a large language model (LLM).\n",
        "\n",
        "We'll use the Oumi framework to streamline the process and achieve high-quality results.\n",
        "\n",
        "We'll cover the following topics:\n",
        "1. Prerequisites\n",
        "2. Data Preparation & Sanity Checks\n",
        "3. Training Config Preparation\n",
        "4. Launching Training\n",
        "5. Monitoring Progress\n",
        "6. Evaluation\n",
        "7. Analyzing Results\n",
        "8. Inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFgU7chY_k_k"
      },
      "source": [
        "# Prerequisites\n",
        "## Hardware\n",
        "The defaults in this tutorial are scaled down for demonstration purposes.\n",
        "\n",
        "The true values are left to code comments within each section.\n",
        "\n",
        "We recommend 8xA100-80GB GPUs to complete in a timely manner with adequate performance.\n",
        "\n",
        "## Oumi Installation\n",
        "\n",
        "First, let's install Oumi. You can find more detailed instructions [here](https://oumi.ai/docs/en/latest/get_started/installation.html).\n",
        "\n",
        "If you have a GPU, you can run the following commands to install Oumi:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "I0uGfy62_k_k",
        "outputId": "0cebb7d5-4b17-4cc4-967c-cf9a102f88af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mResolved \u001b[1m147 packages\u001b[0m \u001b[2min 1.82s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m13 packages\u001b[0m \u001b[2min 583ms\u001b[0m\u001b[0m\n",
            "\u001b[2mInstalled \u001b[1m13 packages\u001b[0m \u001b[2min 230ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.1.3.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.0.2.54\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.2.106\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.4.5.107\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.1.0.106\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.20.5\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.1.105\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.5.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.4.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.20.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.19.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.0.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%pip install uv -q\n",
        "!uv pip install oumi[gpu] --no-progress --system"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm --no-cache-dir"
      ],
      "metadata": {
        "collapsed": true,
        "id": "f4248u3VHWbN",
        "outputId": "bc8a8161-b78c-4c37-b873-62045124aea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vllm in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.45.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.45.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.20.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (4.25.6)\n",
            "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.115.8)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.11)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.59.9)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.11/dist-packages (from vllm) (0.34.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.9.2)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (10.3.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (7.0.2)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.7.0)\n",
            "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.9)\n",
            "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.11)\n",
            "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.2.2)\n",
            "Requirement already satisfied: xgrammar>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.11)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.12.2)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.17.0)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.1.1.post5)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm) (24.0.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.6.1)\n",
            "Requirement already satisfied: mistral_common>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.0->vllm) (1.5.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.0)\n",
            "Requirement already satisfied: compressed-tensors==0.9.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\n",
            "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.18.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (2.41.0)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.560.30 in /usr/local/lib/python3.11/dist-packages (from vllm) (12.560.30)\n",
            "Collecting torch==2.5.1 (from vllm)\n",
            "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.20.1 (from vllm)\n",
            "  Downloading torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: xformers==0.0.28.post3 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.0.28.post3)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
            "Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.5)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
            "Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (20241001)\n",
            "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (2024.9.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->vllm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->vllm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->vllm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->vllm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->vllm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->vllm)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->vllm)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (12.4.127)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1->vllm)\n",
            "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->vllm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->vllm) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.45.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm) (24.2)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.0->vllm) (4.11.0.86)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.23.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->ray[default]>=2.9->vllm) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->ray[default]>=2.9->vllm) (1.5.0)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (0.5.6)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (0.11.4)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (7.1.0)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (20.29.1)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (0.4.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]>=2.9->vllm) (1.70.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.4.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2024.12.14)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.19.1->vllm) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.2->vllm) (0.5.2)\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.11/dist-packages (from xgrammar>=0.1.6->vllm) (2.13.6)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from xgrammar>=0.1.6->vllm) (8.3.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm) (3.21.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm) (14.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.52.0->vllm) (1.0.7)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.22.3)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.9->vllm) (0.3.9)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.9->vllm) (4.3.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]>=2.9->vllm) (0.1.3)\n",
            "Requirement already satisfied: six~=1.16 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]>=2.9->vllm) (1.17.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]>=2.9->vllm) (2.19.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->xgrammar>=0.1.6->vllm) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->xgrammar>=0.1.6->vllm) (1.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open->ray[default]>=2.9->vllm) (1.17.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (1.26.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (2.27.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.9->vllm) (0.6.1)\n",
            "Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m193.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m166.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m237.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m220.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m228.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m185.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m203.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m227.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m201.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m214.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m222.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m315.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m238.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1\n",
            "    Uninstalling torch-2.4.1:\n",
            "      Successfully uninstalled torch-2.4.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.1\n",
            "    Uninstalling torchvision-0.19.1:\n",
            "      Successfully uninstalled torchvision-0.19.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "oumi 0.1.3 requires torch<2.5.0,>=2.4.0, but you have torch 2.5.1 which is incompatible.\n",
            "oumi 0.1.3 requires torchvision<0.20,>=0.19.0, but you have torchvision 0.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 torchvision-0.20.1 triton-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchvision",
                  "triton"
                ]
              },
              "id": "6dcfcd2e93f941e688b77aecefefebc3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF1WrpLi_k_l"
      },
      "source": [
        "**WARNING:** After the first `pip install`, you may have to restart the notebook for the package updates to take effect (Colab Menu: `Runtime` -> `Restart Session`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k92i_-W_k_l"
      },
      "source": [
        "## Creating our working directory\n",
        "For our experiments, we'll use the following folder to save the model, training artifacts, and our working configs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NG4eeXqT_k_l"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "tutorial_dir = \"distillation_tutorial\"\n",
        "\n",
        "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lew7g_t9_k_m"
      },
      "source": [
        "## Setup the environment\n",
        "\n",
        "We'll need to set the following environment variables:\n",
        "- [Optional] HF_TOKEN: Your [HuggingFace](https://huggingface.co/docs/hub/en/security-tokens) token, in case you want to access a private model.\n",
        "- [Optional] WANDB_API_KEY: Your [wandb](https://wandb.ai) token, in case you want to log your experiments to wandb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFYnM_0G_k_m"
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8j-yJ7c_k_m"
      },
      "source": [
        "## Model Download\n",
        "\n",
        "For our purposes it will be much faster if we download our models first.\n",
        "\n",
        "We'll use the `hf_transfer` package to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H2w20qUF_k_n",
        "outputId": "96639e9d-2094-4ea7-af25-00f6f5402e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OrRyLsz7_k_n",
        "outputId": "3bbdef93-243d-4d02-813c-ff06766a0a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/6393b7559e403fd1d80bfead361586fd6f630a4d\n"
          ]
        }
      ],
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 \\\n",
        "    huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
        "    --exclude original/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ROOpBOS0_k_n",
        "outputId": "d9b370c3-ab68-4464-e743-125e67d54e94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/ebf7e8d03db3d86a442d22d30d499abb7ec27bea\n"
          ]
        }
      ],
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 \\\n",
        "    huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Llama-8B \\\n",
        "    --exclude original/*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 \\\n",
        "    huggingface-cli download unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF \\\n",
        "    --exclude original/*"
      ],
      "metadata": {
        "id": "g6Yot48hN5Er",
        "outputId": "cd7dacb0-56cb-4255-b25b-b898e21d57b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading '.gitattributes' to '/root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/a62d45a541d9ae81e9327e920e0ea6b5d293b6ba.incomplete'\n",
            ".gitattributes: 100% 2.06k/2.06k [00:00<00:00, 13.6MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/a62d45a541d9ae81e9327e920e0ea6b5d293b6ba\n",
            "Downloading 'DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf' to '/root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/e18142b69b2dbdac59eca6bf77dde2054078003bcb9534e02e7ca1cf26eb5675.incomplete'\n",
            "DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf: 100% 753M/753M [00:07<00:00, 101MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/e18142b69b2dbdac59eca6bf77dde2054078003bcb9534e02e7ca1cf26eb5675\n",
            "Downloading 'DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L.gguf' to '/root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/6f6c9b97adba11427b13bc159a68881d13d99219165b3bff1b009b27018723db.incomplete'\n",
            "(‚Ä¶)eepSeek-R1-Distill-Qwen-1.5B-Q2_K_L.gguf: 100% 808M/808M [00:11<00:00, 71.8MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/6f6c9b97adba11427b13bc159a68881d13d99219165b3bff1b009b27018723db\n",
            "Downloading 'DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf' to '/root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/1bf6d8e3b6159186e0bf268d52bb22759a284cae319c6544712cfb2bdfb6d01f.incomplete'\n",
            "(‚Ä¶)eepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf: 100% 924M/924M [00:05<00:00, 175MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/1bf6d8e3b6159186e0bf268d52bb22759a284cae319c6544712cfb2bdfb6d01f\n",
            "Downloading 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf' to '/root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/c2c43b6018cf7700ce0ddee8807deb1a9a26758ef878232f3a142d16df81f0fe.incomplete'\n",
            "(‚Ä¶)eepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf: 100% 1.12G/1.12G [00:11<00:00, 96.2MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/c2c43b6018cf7700ce0ddee8807deb1a9a26758ef878232f3a142d16df81f0fe\n",
            "Downloading 'DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M.gguf' to '/root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/ecfbcae4b272bbbe7bfb1b8449d7fea0448a99a87c25f6321b9dd48edba58446.incomplete'\n",
            "(‚Ä¶)eepSeek-R1-Distill-Qwen-1.5B-Q5_K_M.gguf: 100% 1.29G/1.29G [00:13<00:00, 93.9MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/ecfbcae4b272bbbe7bfb1b8449d7fea0448a99a87c25f6321b9dd48edba58446\n",
            "Downloading 'DeepSeek-R1-Distill-Qwen-1.5B-Q6_K.gguf' to '/root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/d700c749e36453767dd65c7e31bf860e7af8fac5eb9b16ea859276e6dbdbd9f5.incomplete'\n",
            "DeepSeek-R1-Distill-Qwen-1.5B-Q6_K.gguf: 100% 1.46G/1.46G [00:09<00:00, 160MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/d700c749e36453767dd65c7e31bf860e7af8fac5eb9b16ea859276e6dbdbd9f5\n",
            "Downloading 'DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf' to '/root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/af3a3fc64d7d0b18f15f28feabad5718d620dc5626a5c1eabad33ddc2b5d09a9.incomplete'\n",
            "DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf: 100% 1.89G/1.89G [00:20<00:00, 91.2MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/af3a3fc64d7d0b18f15f28feabad5718d620dc5626a5c1eabad33ddc2b5d09a9\n",
            "Downloading 'README.md' to '/root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/739a38fd3f86b86c04444cdfce3eb8370661f109.incomplete'\n",
            "README.md: 100% 24.7k/24.7k [00:00<00:00, 98.1MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blobs/739a38fd3f86b86c04444cdfce3eb8370661f109\n",
            "/root/.cache/huggingface/hub/models--unsloth--DeepSeek-R1-Distill-Qwen-1.5B-GGUF/snapshots/9784122b3247cc074b19c42bf38ee256d8aacce7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ptPSu7T_k_n"
      },
      "source": [
        "## Baseline Evals\n",
        "\n",
        "Before we can improve our small model, we should measure how well it performs on a benchmark compared to the larger model.\n",
        "\n",
        "The below code will run the MMLU PRO Math task from LM Harness.\n",
        "\n",
        "Note that this will take some time, so we've recorded our results below for your convenience:\n",
        "\n",
        "| Model | MMLU Pro Math Accuracy |\n",
        "|-------|------------------------|\n",
        "| R1 Distill 1.5B | 38.49% +- 1.32% |\n",
        "| R1 Distill 70B | 61.07% +- 1.33% |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf8UBrZH_k_n"
      },
      "source": [
        "### Run Evals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "scLrRSJC_k_n",
        "outputId": "654a4fc8-8411-43da-e319-e26bc69a7d2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting distillation_tutorial/eval_small.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile $tutorial_dir/eval_small.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  # shard_for_eval: True # Uncomment this line for multi-gpu setups.\n",
        "\n",
        "\n",
        "tasks:\n",
        "  - evaluation_platform: lm_harness\n",
        "    task_name: mmlu_pro_math\n",
        "\n",
        "output_dir: \"distillation_tutorial/output/evaluation\"\n",
        "generation:\n",
        "  batch_size: 1 # LM Harness recommends BS=1 for reproducibility.\n",
        "  # batch_size: 256  # Replace with 256 for 8xA100-80GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MVQSTJk8_k_o",
        "outputId": "e2a99e85-7571-455c-b126-24148d667037",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "@                 @\n",
            "@   @@@@@  @  @   @\n",
            "@   @   @  @  @   @\n",
            "@   @@@@@  @@@@   @\n",
            "@                 @\n",
            "@   @@@@@@@   @   @\n",
            "@   @  @  @   @   @\n",
            "@   @  @  @   @   @\n",
            "@                 @\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "2025-02-01 15:08:29.849362: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738422510.075148    5946 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738422510.138723    5946 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-01 15:08:30.612050: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[2025-02-01 15:08:40,082][oumi][rank0][pid:5946][MainThread][INFO]][lm_harness.py:110] Starting evaluation...\n",
            "[2025-02-01 15:08:40,082][oumi][rank0][pid:5946][MainThread][INFO]][lm_harness.py:111] \tLM Harness `model_params`:\n",
            "{'device_map': 'auto',\n",
            " 'dtype': torch.bfloat16,\n",
            " 'parallelize': False,\n",
            " 'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B',\n",
            " 'trust_remote_code': False}\n",
            "[2025-02-01 15:08:40,082][oumi][rank0][pid:5946][MainThread][INFO]][lm_harness.py:112] \tLM Harness `task_params`:\n",
            "LMHarnessTaskParams(evaluation_platform='lm_harness',\n",
            "                    task_name='mmlu_pro_math',\n",
            "                    num_samples=None,\n",
            "                    eval_kwargs={},\n",
            "                    num_fewshot=None)\n",
            "INFO:lm-eval:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm-eval:Initializing hf model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'trust_remote_code': False, 'parallelize': False, 'dtype': torch.bfloat16, 'device_map': 'auto'}\n",
            "INFO:lm-eval:Using device 'cuda:0'\n",
            "INFO:lm-eval:Model parallel was set to False.\n",
            "README.md: 100% 10.9k/10.9k [00:00<00:00, 38.3MB/s]\n",
            "test-00000-of-00001.parquet: 100% 4.15M/4.15M [00:00<00:00, 47.2MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 45.3k/45.3k [00:00<00:00, 147MB/s]\n",
            "Generating test split: 100% 12032/12032 [00:00<00:00, 163657.33 examples/s]\n",
            "Generating validation split: 100% 70/70 [00:00<00:00, 25059.86 examples/s]\n",
            "Filter: 100% 70/70 [00:00<00:00, 14304.57 examples/s]\n",
            "Filter: 100% 12032/12032 [00:00<00:00, 57041.82 examples/s]\n",
            "INFO:lm-eval:Building contexts for mmlu_pro_math on rank 0...\n",
            "100% 1351/1351 [00:01<00:00, 726.11it/s]\n",
            "INFO:lm-eval:Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1351 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "Running generate_until requests:   2% 21/1351 [04:22<4:36:55, 12.49s/it]\n"
          ]
        }
      ],
      "source": [
        "!oumi evaluate -c \"$tutorial_dir/eval_small.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mhd2aGg2_k_o",
        "outputId": "db485682-ec18-4d3d-fa12-fabba8dc4daf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting distillation_tutorial/eval_large.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile $tutorial_dir/eval_large.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  shard_for_eval: True\n",
        "\n",
        "\n",
        "tasks:\n",
        "  - evaluation_platform: lm_harness\n",
        "    task_name: mmlu_pro_math\n",
        "\n",
        "output_dir: \"distillation_tutorial/output/evaluation\"\n",
        "generation:\n",
        "  batch_size: 1 # LM Harness recommends BS=1 for reproducibility.\n",
        "  # batch_size: 64  # Replace with 64 for 8xA100-80GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "HwRaKbId_k_o",
        "outputId": "8e005986-c409-4bdf-b37b-64e16c835a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "@                 @\n",
            "@   @@@@@  @  @   @\n",
            "@   @   @  @  @   @\n",
            "@   @@@@@  @@@@   @\n",
            "@                 @\n",
            "@   @@@@@@@   @   @\n",
            "@   @  @  @   @   @\n",
            "@   @  @  @   @   @\n",
            "@                 @\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "\n",
            "2025-02-01 15:14:07.170006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738422847.189999    7454 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738422847.196809    7454 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-01 15:14:07.218904: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[2025-02-01 15:14:11,326][oumi][rank0][pid:7454][MainThread][INFO]][lm_harness.py:110] Starting evaluation...\n",
            "[2025-02-01 15:14:11,326][oumi][rank0][pid:7454][MainThread][INFO]][lm_harness.py:111] \tLM Harness `model_params`:\n",
            "{'device_map': 'auto',\n",
            " 'dtype': torch.bfloat16,\n",
            " 'parallelize': True,\n",
            " 'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',\n",
            " 'trust_remote_code': False}\n",
            "[2025-02-01 15:14:11,326][oumi][rank0][pid:7454][MainThread][INFO]][lm_harness.py:112] \tLM Harness `task_params`:\n",
            "LMHarnessTaskParams(evaluation_platform='lm_harness',\n",
            "                    task_name='mmlu_pro_math',\n",
            "                    num_samples=None,\n",
            "                    eval_kwargs={},\n",
            "                    num_fewshot=None)\n",
            "INFO:lm-eval:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm-eval:Initializing hf model, with arguments: {'pretrained': 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B', 'trust_remote_code': False, 'parallelize': True, 'dtype': torch.bfloat16, 'device_map': 'auto'}\n",
            "INFO:lm-eval:Using `accelerate launch` or `parallelize=True`, device 'cuda:0' will be overridden when placing model.\n",
            "INFO:lm-eval:Model parallel was set to True, setting max memory per GPU to {0: 15718285312} and device map to auto\n",
            "Loading checkpoint shards: 100% 2/2 [01:05<00:00, 32.57s/it]\n",
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk.\n",
            "INFO:lm-eval:Building contexts for mmlu_pro_math on rank 0...\n",
            "100% 1351/1351 [00:01<00:00, 744.65it/s]\n",
            "INFO:lm-eval:Running generate_until requests\n",
            "Running generate_until requests:   0% 0/1351 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/oumi\", line 10, in <module>\n",
            "    sys.exit(run())\n",
            "             ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/oumi/cli/main.py\", line 103, in run\n",
            "    return app()\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/main.py\", line 340, in __call__\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/main.py\", line 323, in __call__\n",
            "    return get_command(self)(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1161, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 743, in main\n",
            "    return _main(\n",
            "           ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 198, in _main\n",
            "    rv = self.invoke(ctx)\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1697, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1443, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 788, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/main.py\", line 698, in wrapper\n",
            "    return callback(**use_params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/oumi/cli/evaluate.py\", line 40, in evaluate\n",
            "    oumi_evaluate(parsed_config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/oumi/__init__.py\", line 109, in evaluate\n",
            "    return oumi.evaluate.evaluate(config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/oumi/evaluate.py\", line 34, in evaluate\n",
            "    result = evaluate_lm_harness(\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/oumi/evaluation/lm_harness.py\", line 113, in evaluate\n",
            "    lm_eval_output = lm_eval.simple_evaluate(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 401, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 303, in simple_evaluate\n",
            "    results = evaluate(\n",
            "              ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 401, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 507, in evaluate\n",
            "    resps = getattr(lm, reqtype)(cloned_reqs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 1323, in generate_until\n",
            "    cont = self._model_generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 876, in _model_generate\n",
            "    return self.model.generate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2047, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3007, in _sample\n",
            "    outputs = self(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 1189, in forward\n",
            "    outputs = self.model(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 1000, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 729, in forward\n",
            "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
            "                                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\", line 655, in forward\n",
            "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 116.12 MiB is free. Process 93420 has 14.62 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 468.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Running generate_until requests:   0% 0/1351 [00:07<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "!oumi evaluate -c \"$tutorial_dir/eval_large.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTeT0Ytw_k_o"
      },
      "source": [
        "## Prepare Inference Data\n",
        "\n",
        "Now that we've set our baseline numbers, let's prepare the training data we'll use to improve 1.5B.\n",
        "\n",
        "Given our goal is to improve MMLU Pro Math performance, we should ideally pick data that's similar.\n",
        "\n",
        "`meta-math/MetaMathQA` is a good choice as it avoids test set contamination while being similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2ulfshCh_k_o",
        "outputId": "60530812-2838-4c52-b5ac-6956faae3389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 02-01 16:11:30 __init__.py:183] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import datasets\n",
        "import torch\n",
        "\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.core.types import Conversation, Message, Role\n",
        "from oumi.inference import VLLMInferenceEngine\n",
        "\n",
        "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
        "# If you're not running in a notebook, you can ignore this.\n",
        "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IaXM1BBT_k_o",
        "outputId": "e21d9530-7a7d-47f2-da69-595599c95b36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gracie and Joe are choosing numbers on the complex plane. Joe chooses the point $1+2i$. Gracie chooses $-1+i$. How far apart are Gracie and Joe's points?\n",
            "num samples:  10000\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.load_dataset(\n",
        "    \"meta-math/MetaMathQA\",\n",
        "    revision=\"aa4f34d\",\n",
        "    split=\"train[:10000]\",  # We'll focus only on the first 10k samples.\n",
        ")\n",
        "\n",
        "data = [sample[\"query\"] for sample in dataset]\n",
        "print(data[0])\n",
        "print(\"num samples: \", len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FrTQxaMI_k_o",
        "outputId": "c642b527-a8ff-4ef9-afd6-8e005b1e44f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conversation_id=None messages=[USER: Gracie and Joe are choosing numbers on the complex plane. Joe chooses the point $1+2i$. Gracie chooses $-1+i$. How far apart are Gracie and Joe's points?] metadata={}\n"
          ]
        }
      ],
      "source": [
        "conversations = [\n",
        "    Conversation(\n",
        "        messages=[\n",
        "            Message(role=Role.USER, content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    for prompt in data\n",
        "]\n",
        "print(conversations[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ufCtBU_k_o"
      },
      "source": [
        "## Run Inference\n",
        "\n",
        "Now that our data is in the right format for collecting responses, let's go ahead and run inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x4vKJdAN_k_o",
        "outputId": "36a3a260-a3ce-4797-b335-5eca63c29801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting distillation_tutorial/infer_large.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile $tutorial_dir/infer_large.yaml\n",
        "\n",
        "model:\n",
        "  #model_name: \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "  #model_name: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "  model_name: \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  model_max_length: 8192\n",
        "\n",
        "generation:\n",
        "  max_new_tokens: 8192"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade oumi[gpu]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VJBsAC-tRB69",
        "outputId": "6c1ff418-1218-4c95-c949-0d4fa436706d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: oumi[gpu] in /usr/local/lib/python3.11/dist-packages (0.1.3)\n",
            "Requirement already satisfied: accelerate<1.3,>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (1.2.1)\n",
            "Requirement already satisfied: aiohttp<3.12,>=3.10 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (3.11.11)\n",
            "Requirement already satisfied: aiofiles<25,>=22.1.0 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (24.1.0)\n",
            "Requirement already satisfied: aioresponses<0.8,>=0.7.6 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (0.7.8)\n",
            "Requirement already satisfied: datasets<3.3,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (3.2.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (4.0.0)\n",
            "Requirement already satisfied: lm_eval<0.5.0,>=0.4.5 in /usr/local/lib/python3.11/dist-packages (from lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (0.4.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (1.26.4)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (2.2.2)\n",
            "Requirement already satisfied: peft<0.15,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (0.14.0)\n",
            "Requirement already satisfied: pexpect<4.9,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (4.8.0)\n",
            "Requirement already satisfied: pillow<10.4,>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (10.3.0)\n",
            "Requirement already satisfied: pydantic<2.10,>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (2.9.2)\n",
            "Requirement already satisfied: responses<0.26,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (0.25.6)\n",
            "Requirement already satisfied: skypilot<0.8,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (0.7.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (2.18.0)\n",
            "Collecting torch<2.5.0,>=2.4.0 (from oumi[gpu])\n",
            "  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: torchdata<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (0.9.0)\n",
            "Collecting torchvision<0.20,>=0.19.0 (from oumi[gpu])\n",
            "  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (4.67.1)\n",
            "Requirement already satisfied: transformers<4.46,>=4.45.2 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (4.45.2)\n",
            "Requirement already satisfied: trl<0.12,>=0.11.4 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (0.11.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (0.15.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (4.12.2)\n",
            "Requirement already satisfied: wandb<0.20,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (0.19.5)\n",
            "Requirement already satisfied: liger-kernel<0.4,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (0.3.1)\n",
            "Requirement already satisfied: nvidia-ml-py<12.561,>=12.560.30 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (12.560.30)\n",
            "Requirement already satisfied: bitsandbytes<0.46,>=0.45.0 in /usr/local/lib/python3.11/dist-packages (from oumi[gpu]) (0.45.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<1.3,>=1.2.1->oumi[gpu]) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate<1.3,>=1.2.1->oumi[gpu]) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<1.3,>=1.2.1->oumi[gpu]) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<1.3,>=1.2.1->oumi[gpu]) (0.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.12,>=3.10->oumi[gpu]) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.12,>=3.10->oumi[gpu]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.12,>=3.10->oumi[gpu]) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.12,>=3.10->oumi[gpu]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.12,>=3.10->oumi[gpu]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.12,>=3.10->oumi[gpu]) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.12,>=3.10->oumi[gpu]) (1.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<3.3,>=3.2.0->oumi[gpu]) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<3.3,>=3.2.0->oumi[gpu]) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<3.3,>=3.2.0->oumi[gpu]) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<3.3,>=3.2.0->oumi[gpu]) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<3.3,>=3.2.0->oumi[gpu]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<3.3,>=3.2.0->oumi[gpu]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<3.3,>=3.2.0->oumi[gpu]) (2024.9.0)\n",
            "Requirement already satisfied: triton>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from liger-kernel<0.4,>=0.3.1->oumi[gpu]) (3.1.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (0.4.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (2.10.2)\n",
            "Requirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (2.13.6)\n",
            "Requirement already satisfied: pytablewriter in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (1.2.1)\n",
            "Requirement already satisfied: rouge-score>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (0.1.2)\n",
            "Requirement already satisfied: sacrebleu>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (2.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (1.6.1)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (2.1.0)\n",
            "Requirement already satisfied: tqdm-multiprocess in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (0.0.11)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (0.23.0)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (1.1)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (10.5.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4,>=2.3.0->oumi[gpu]) (4.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=2.0.3->oumi[gpu]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=2.0.3->oumi[gpu]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=2.0.3->oumi[gpu]) (2025.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect<4.9,>=4.8.0->oumi[gpu]) (0.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.10,>=2.9.2->oumi[gpu]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.10,>=2.9.2->oumi[gpu]) (2.23.4)\n",
            "Requirement already satisfied: urllib3<3.0,>=1.25.10 in /usr/local/lib/python3.11/dist-packages (from responses<0.26,>=0.25.0->oumi[gpu]) (2.3.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (0.45.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (5.5.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (8.1.8)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (0.4.6)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (43.0.3)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (3.1.5)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (4.23.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (3.4.2)\n",
            "Requirement already satisfied: pendulum in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (3.0.0)\n",
            "Requirement already satisfied: PrettyTable>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (3.13.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (1.0.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (13.9.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (0.9.0)\n",
            "Requirement already satisfied: pulp in /usr/local/lib/python3.11/dist-packages (from skypilot<0.8,>=0.7.0->oumi[gpu]) (2.9.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18.0->oumi[gpu]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18.0->oumi[gpu]) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18.0->oumi[gpu]) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18.0->oumi[gpu]) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18.0->oumi[gpu]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18.0->oumi[gpu]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18.0->oumi[gpu]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18.0->oumi[gpu]) (3.1.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<2.5.0,>=2.4.0->oumi[gpu]) (1.13.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.5.0,>=2.4.0->oumi[gpu])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.5.0,>=2.4.0->oumi[gpu])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.5.0,>=2.4.0->oumi[gpu])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<2.5.0,>=2.4.0->oumi[gpu]) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.5.0,>=2.4.0->oumi[gpu])\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.5.0,>=2.4.0->oumi[gpu])\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.5.0,>=2.4.0->oumi[gpu])\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.5.0,>=2.4.0->oumi[gpu])\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.5.0,>=2.4.0->oumi[gpu])\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<2.5.0,>=2.4.0->oumi[gpu])\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.5.0,>=2.4.0->oumi[gpu])\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton>=2.3.0 (from liger-kernel<0.4,>=0.3.1->oumi[gpu])\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.5.0,>=2.4.0->oumi[gpu]) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.46,>=4.45.2->oumi[gpu]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/dist-packages (from transformers<4.46,>=4.45.2->oumi[gpu]) (0.20.3)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.11/dist-packages (from trl<0.12,>=0.11.4->oumi[gpu]) (0.9.13)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb<0.20,>=0.19.3->oumi[gpu]) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb<0.20,>=0.19.3->oumi[gpu]) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb<0.20,>=0.19.3->oumi[gpu]) (4.3.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb<0.20,>=0.19.3->oumi[gpu]) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb<0.20,>=0.19.3->oumi[gpu]) (1.3.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->oumi[gpu]) (1.5.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb<0.20,>=0.19.3->oumi[gpu]) (4.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.0->skypilot<0.8,>=0.7.0->oumi[gpu]) (3.0.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from PrettyTable>=2.0.0->skypilot<0.8,>=0.7.0->oumi[gpu]) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<3.3,>=3.2.0->oumi[gpu]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<3.3,>=3.2.0->oumi[gpu]) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<3.3,>=3.2.0->oumi[gpu]) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->skypilot<0.8,>=0.7.0->oumi[gpu]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->skypilot<0.8,>=0.7.0->oumi[gpu]) (2.18.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (3.9.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (3.1.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (5.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (3.5.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl<0.12,>=0.11.4->oumi[gpu]) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl<0.12,>=0.11.4->oumi[gpu]) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl<0.12,>=0.11.4->oumi[gpu]) (4.4.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->skypilot<0.8,>=0.7.0->oumi[gpu]) (1.17.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->skypilot<0.8,>=0.7.0->oumi[gpu]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->skypilot<0.8,>=0.7.0->oumi[gpu]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->skypilot<0.8,>=0.7.0->oumi[gpu]) (0.22.3)\n",
            "Requirement already satisfied: time-machine>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from pendulum->skypilot<0.8,>=0.7.0->oumi[gpu]) (2.16.0)\n",
            "Requirement already satisfied: DataProperty<2,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (1.1.0)\n",
            "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (1.1.4)\n",
            "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (3.2.3)\n",
            "Requirement already satisfied: tabledata<2,>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (1.3.4)\n",
            "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (0.1.7)\n",
            "Requirement already satisfied: typepy<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (1.3.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<2.5.0,>=2.4.0->oumi[gpu]) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->skypilot<0.8,>=0.7.0->oumi[gpu]) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<0.20,>=0.19.3->oumi[gpu]) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->skypilot<0.8,>=0.7.0->oumi[gpu]) (0.1.2)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval<0.5.0,>=0.4.5->lm_eval[wandb]<0.5.0,>=0.4.5->oumi[gpu]) (5.2.0)\n",
            "Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1\n",
            "    Uninstalling torch-2.5.1:\n",
            "      Successfully uninstalled torch-2.5.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1\n",
            "    Uninstalling torchvision-0.20.1:\n",
            "      Successfully uninstalled torchvision-0.20.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "vllm 0.7.0 requires torch==2.5.1, but you have torch 2.4.1 which is incompatible.\n",
            "vllm 0.7.0 requires torchvision==0.20.1, but you have torchvision 0.19.1 which is incompatible.\n",
            "xformers 0.0.28.post3 requires torch==2.5.1, but you have torch 2.4.1 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 torchvision-0.19.1 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "functorch",
                  "torch",
                  "torchgen",
                  "torchvision",
                  "triton"
                ]
              },
              "id": "79ef37197ad145e6a9749b681b6d8c5f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HlkkzP1d_k_o",
        "outputId": "52161213-e4ba-4492-aa4c-413c71ae5b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'oumi.utils.transformers_utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'oumi.utils.transformers_utils'"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Download, and load the model in memory\n",
        "# This may take a while, depending on your internet speed.\n",
        "# The inference engine only needs to be loaded once and can be\n",
        "# reused for multiple conversations.\n",
        "config_path = f\"{tutorial_dir}/infer_large.yaml\"\n",
        "config = InferenceConfig.from_yaml(config_path)\n",
        "\n",
        "from oumi.utils.transformers_utils import build_tokenizer\n",
        "\n",
        "# Build the tokenizer with the specified model type\n",
        "tokenizer = build_tokenizer(config.model, model_type=\"qwen\") # Passing model_type here\n",
        "\n",
        "inference_engine = VLLMInferenceEngine(\n",
        "    config.model,\n",
        "    #tensor_parallel_size=torch.cuda.device_count(),  # use all available GPUs\n",
        "    tensor_parallel_size=1,\n",
        "    # Enable prefix caching for vLLM.\n",
        "    # This is key for performance when running prompts with a long prefix,\n",
        "    # such as judging or conversations with large system prompts\n",
        "    # or few-shot examples.\n",
        "    enable_prefix_caching=True,\n",
        "    # Use quantization to reduce memory footprint\n",
        "    #quantization=\"gguf\", # Add this line to enable quantization\n",
        "    tokenizer=tokenizer # Pass the tokenizer to VLLMInferenceEngine\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clO1orRD_k_p"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "print(f\"Running inference for {len(conversations)} conversations\")\n",
        "\n",
        "generations = inference_engine.infer(\n",
        "    input=conversations,\n",
        "    inference_config=config,\n",
        ")\n",
        "print(generations[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8c6Tmg__k_p"
      },
      "source": [
        "## Prepare Training Data\n",
        "\n",
        "Now that we've finished collecting responses, let's go ahead and prepare the data for training and save it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByBTDY1d_k_p"
      },
      "outputs": [],
      "source": [
        "conversation_dicts = [c.to_dict() for c in generations]\n",
        "print(conversation_dicts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtcsBkNI_k_p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataframe = pd.DataFrame(conversation_dicts)\n",
        "print(dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8yPWJyu_k_p"
      },
      "outputs": [],
      "source": [
        "dataframe.to_json(f\"{tutorial_dir}/math_train_10k.jsonl\", orient=\"records\", lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzarQAZT_k_p"
      },
      "source": [
        "## Run Distillation\n",
        "\n",
        "Now that the data is ready, we can begin distilling the model. For this form of distillation, we will be fully fine-tuning the model with supervised fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mEBQqOo_k_p"
      },
      "outputs": [],
      "source": [
        "%%writefile $tutorial_dir/train.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "  trust_remote_code: true\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  device_map: \"auto\"\n",
        "\n",
        "data:\n",
        "  train:\n",
        "    datasets:\n",
        "      - dataset_name: \"text_sft_jsonl\"\n",
        "        dataset_path: \"./distillation_tutorial/math_train_10k.jsonl\"\n",
        "        split: \"train\"\n",
        "        shuffle: True\n",
        "        seed: 42\n",
        "    seed: 42\n",
        "\n",
        "training:\n",
        "  output_dir: \"distillation_tutorial/output/finetune\"\n",
        "\n",
        "  # For a single GPU, the following gives us a batch size of 16\n",
        "  # If training with multiple GPUs, feel free to reduce gradient_accumulation_steps\n",
        "  per_device_train_batch_size: 2\n",
        "  gradient_accumulation_steps: 1  # Reduce this to 1 for 8xA100-80GB GPUs\n",
        "\n",
        "  # ***NOTE***\n",
        "  # We set it to 10 steps to first verify that it works\n",
        "  # Comment out the line below to have it train for 1 full epoch (all the data) instead.\n",
        "  # Note: 1 full epoch will take about 13 minutes on 8xA100-80GB.\n",
        "  max_steps: 10\n",
        "\n",
        "  num_train_epochs: 1\n",
        "  learning_rate: 1e-4\n",
        "  warmup_ratio: 0.1\n",
        "  logging_steps: 10\n",
        "  save_steps: 0\n",
        "  max_grad_norm: 10\n",
        "  weight_decay: 0.01\n",
        "\n",
        "\n",
        "  trainer_type: \"TRL_SFT\"\n",
        "  optimizer: \"adamw_torch_fused\"\n",
        "  enable_gradient_checkpointing: True\n",
        "  gradient_checkpointing_kwargs:\n",
        "    use_reentrant: False\n",
        "  ddp_find_unused_parameters: False\n",
        "  dataloader_num_workers: \"auto\"\n",
        "  dataloader_prefetch_factor: 32\n",
        "  empty_device_cache_steps: 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NOuvVLI_k_p"
      },
      "source": [
        "### Single GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJeoXMbt_k_p"
      },
      "outputs": [],
      "source": [
        "!oumi train -c \"$tutorial_dir/train.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3SK59Dc_k_p"
      },
      "source": [
        "### Multi-GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHACguM2_k_p"
      },
      "outputs": [],
      "source": [
        "!oumi distributed torchrun -m oumi train -c \"$tutorial_dir/train.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtLAoHa9_k_p"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "Now that we have a new distilled model, let's evaluate it on the same benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf7gt0t8_k_q"
      },
      "outputs": [],
      "source": [
        "%%writefile $tutorial_dir/eval_small_fft.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"./distillation_tutorial/output/finetune/\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  shard_for_eval: True\n",
        "\n",
        "\n",
        "tasks:\n",
        "  - evaluation_platform: lm_harness\n",
        "    task_name: mmlu_pro_math\n",
        "\n",
        "output_dir: \"distillation_tutorial/output/evaluation\"\n",
        "generation:\n",
        "  batch_size: 1 # LM Harness recommends BS=1 for reproducibility.\n",
        "  # batch_size: 256  # Replace with 256 for 8xA100-80GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1D2RU0u_k_v"
      },
      "outputs": [],
      "source": [
        "!oumi evaluate -c \"$tutorial_dir/eval_small_fft.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESj0FH96_k_v"
      },
      "source": [
        "## Results\n",
        "\n",
        "After we finetuned the model following the steps above, we achieved the following results:\n",
        "\n",
        "| Model           | Accuracy        |\n",
        "|-----------------|-----------------|\n",
        "| R1 Distill 1.5B | 38.49% +- 1.32% |\n",
        "| Oumi R1 Distill 1.5B | 42.41% +- 1.34% |\n",
        "| R1 Distill 70B  | 61.07% +- 1.33% |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}